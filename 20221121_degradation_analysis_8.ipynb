{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "121113bc",
   "metadata": {},
   "source": [
    "# Degradation Data Clustering\n",
    "\n",
    "Titan Hartono (titan.hartono@helmholtz-berlin.de)\n",
    "Data collected and cleaned from: Paolo Graniero, Hans Koebler\n",
    "\n",
    "ver 20221122\n",
    "\n",
    "## 1. Import libraries and load the dataset\n",
    "\n",
    "For this section,\n",
    "\n",
    "**input**: .pkl datasetS that was converted from raw .json data in previous notebook\n",
    "\n",
    "**process**:\n",
    "1. load the libraries\n",
    "2. filter out bad pixels\n",
    "3. filter out the one with empty MPPT_EFF\n",
    "4. drop the data with length < 150 hours\n",
    "5. cut the series to only MPPT_t and MPPT_EFF (there are other things in the dataset, but we're focusing on the MPPT_EFF for now)\n",
    "6. resample to every 10 minutes \n",
    "7. drop the length to 150 hours, hence: 1 hour = 6 data points, 150 hours = 900 data points\n",
    "8. interpolate if there's a NaN values (selected method: akima)\n",
    "\n",
    "**output**: one whole .pkl file for good, further-processable data\n",
    "\n",
    "### 1.1. Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8195ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the following packages if they haven't been installed\n",
    "\n",
    "# pip install \"dask[complete]\"\n",
    "# pip install \"-U kaleido\"\n",
    "# pip install \"kaleido\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a2d03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the packages needed for the notebook to run\n",
    "\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "# import rdkit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, read_csv\n",
    "from IPython.display import display_html\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pickle5 as pickle\n",
    "import dask\n",
    "\n",
    "from minisom import MiniSom\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f81f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up name to save files\n",
    "filedirname = '20221121_run/sigma_0p5_learningrate_0p6/20221121_sigma_0p5_learningrate_0p6_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c1b4d0",
   "metadata": {},
   "source": [
    "### 1.2. Load dataset in pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99be76bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lOAD PICKLE FOR WHOLE SERIES\n",
    "\n",
    "# Go to the dataset\n",
    "directory = 'dataset/pkl_3/'\n",
    "\n",
    "# for pkl_1\n",
    "# wholeSeries = pd.DataFrame(columns=['SampleNumber', 'Pixel', 'Filename', 'Temperature', 'Irradiation',\n",
    "#        'MPPTdata', 'IVdataFor', 'IVdataRev', 'Area', 'Atmosphere', 'Filter',\n",
    "#        'Samplename', 'Parameter_Nr', 'Parameter_Name', 'Experiment_Name',\n",
    "#        'T_set', 'FrontContact', 'CSL1', 'CSL2', 'ABS', 'CSL3', 'CSL4',\n",
    "#        'BackContact', 'Box_used', 'Pixelfilter', 'Bestpix'])\n",
    "\n",
    "# for pkl_2\n",
    "wholeSeries = pd.DataFrame(columns=['SampleNumber', 'Pixel', 'Filename', 'Temperature', 'Irradiation',\n",
    "                                    'ShutterAction', 'MPPTdata', 'IVdataFor', 'IVdataRev',\n",
    "                                    'dark_MPPTdata', 'dark_IVdataFor', 'dark_IVdataRev', 'Area',\n",
    "                                    'Atmosphere', 'Filter', 'Box_used', 'Experiment_type', 'Samplename',\n",
    "                                    'Parameter_Nr', 'Parameter_Name', 'Experiment_Name', 'T_set',\n",
    "                                    'FrontContact', 'CSL1', 'CSL2', 'ABS', 'CSL3', 'CSL4', 'BackContact',\n",
    "                                    'Pixelfilter', 'Bestpix'])\n",
    "\n",
    "namesofMySeries = []\n",
    "\n",
    "# Load the files from the directories\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".pkl\"):\n",
    "        \n",
    "        # Oopen the directory and load the files\n",
    "        with open(directory+filename, \"rb\") as fh:\n",
    "            df = pickle.load(fh)\n",
    "        print(filename)\n",
    "\n",
    "        wholeSeries = wholeSeries.append(df) \n",
    "        # df = df.loc[:,[\"date\",\"value\"]]\n",
    "\n",
    "        # While we are at it I just filtered the columns that we will be working on\n",
    "        # df.set_index(\"date\",inplace=True)\n",
    "\n",
    "        # ,set the date columns as index\n",
    "        # df.sort_index(inplace=True)\n",
    "\n",
    "        # and lastly, ordered the data according to our date index\n",
    "        # mySeries.append(df)\n",
    "        # namesofMySeries.append(filename[:-4])\n",
    "\n",
    "# Reindexing\n",
    "wholeSeries.index = range(wholeSeries.shape[0])\n",
    "wholeSeries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe590f0",
   "metadata": {},
   "source": [
    "### 1.3. Filter out bad devices, empty rows, and data that is too short\n",
    "Next: not all devices are good. We are adding the pixel information from 'Pixelfilter' (if it's bad or good), labeled by the researcher fabricating the devices directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b6d6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adding the pixelfilter as a column for each device\n",
    "list_pixel = []\n",
    "print('Series length: ',len(wholeSeries)/6) # Series: cells, pixel: *6 of the cells\n",
    "\n",
    "# Adding the pixel number to the list\n",
    "for i in range(int(len(wholeSeries)/6)):\n",
    "    for k in range(6):\n",
    "        list_pixel.append((wholeSeries['Pixelfilter'][i*6])[k])\n",
    "\n",
    "# Adding the list to the wholeSeries dataframe\n",
    "wholeSeries['Pixelfilter_ind'] = list_pixel\n",
    "\n",
    "# Get names of indexes for which column pixelfilter is 0\n",
    "indexNames = wholeSeries[wholeSeries['Pixelfilter_ind'] == 0 ].index\n",
    "\n",
    "# Delete these row indexes from dataFrame\n",
    "wholeSeries.drop(indexNames , inplace=True)\n",
    "\n",
    "# Reindexing\n",
    "wholeSeries.index = range(wholeSeries.shape[0])\n",
    "wholeSeries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498720b6",
   "metadata": {},
   "source": [
    "Next: \n",
    "1. Drop the row with no MPPTdata\n",
    "2. Drop the row with data length < hour limit (in our case: 150 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde5acf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Next: drop row if there is no MPPTdata\n",
    "indexDrop = []\n",
    "hour_limit =  150 #hours\n",
    "wholeSeries['T_avg']=0.0\n",
    "wholeSeries['Irr_avg']=0.0\n",
    "\n",
    "for num in range(len(wholeSeries)):\n",
    "    # Add column of average irradiation and temperature ## ADD THIS TEMPERATURE LATER\n",
    "#     wholeSeries['T_avg'][num] = wholeSeries['Temperature'][num]['Temperature'].mean()\n",
    "#     wholeSeries['Irr_avg'][num] = wholeSeries['Irradiation'][num]['Irradiation'].mean()\n",
    "    \n",
    "    # Drop row if there is no MPPTdata or MPPTdata<hour_limit\n",
    "    if len(wholeSeries['MPPTdata'][num])==0: # or max(wholeSeries['MPPTdata'][num]['MPPT_dur_h'])< hour_limit:\n",
    "        indexDrop.append(num)\n",
    "\n",
    "print('# gets dropped due to empty MPPTdata: ', len(indexDrop))\n",
    "\n",
    "# Drop based on the condition\n",
    "mySeries = wholeSeries.drop(indexDrop, axis=0, inplace=False)\n",
    "mySeries.reset_index(drop=True, inplace=True)\n",
    "print('# rows surviving: ', len(mySeries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a542af35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the data length variations\n",
    "max_hour = []\n",
    "\n",
    "for i in range(len(mySeries)):\n",
    "    max_hour.append(max(mySeries['MPPTdata'][i]['MPPT_dur_h']))\n",
    "\n",
    "# Plotting the distribution\n",
    "sns.set_style('darkgrid')\n",
    "sns.set(rc={'figure.figsize':(5,4)})\n",
    "sns.distplot(np.array((max_hour)),kde=False,\n",
    "#              bins=[150,300,450,600,750,900,1050,1200,1350,1500,1650,1800,1950,2100])\n",
    "             bins=[50,100,150,200,250,300,350,400,450,500,550,600,650,700,750,800,850,900,950,1000,\n",
    "                   1050,1100,1150,1200,1250,1300,1350,1400,1450,1500,\n",
    "                   1550,1600,1650,1700,1750,1800,1850,1900,1950,2000,2050,2100])\n",
    "\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.xlabel('Degradation length (hours)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(filedirname+'distribution_hour.png', dpi=600)\n",
    "\n",
    "# Print the min and max hour\n",
    "print('min hour: ',min(np.array(max_hour)))\n",
    "print('max hour: ',max(np.array(max_hour)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad7064c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Next: drop row if the MPPTdata is too short (< hour_limit)\n",
    "indexDrop = []\n",
    "hour_limit =  150 #hours\n",
    "mySeries['T_avg']=0.0\n",
    "mySeries['Irr_avg']=0.0\n",
    "\n",
    "for num in range(len(mySeries)):\n",
    "    \n",
    "    # Drop row if MPPTdata<hour_limit\n",
    "    if max(mySeries['MPPTdata'][num]['MPPT_dur_h'])< hour_limit:\n",
    "        indexDrop.append(num)\n",
    "\n",
    "print('# gets dropped due to short MPPTdata: ', len(indexDrop))\n",
    "\n",
    "# Drop based on the condition\n",
    "mySeries = mySeries.drop(indexDrop, axis=0, inplace=False)\n",
    "mySeries.reset_index(drop=True, inplace=True)\n",
    "print('# rows surviving: ', len(mySeries))\n",
    "\n",
    "# Reindexing\n",
    "mySeries.index = range(mySeries.shape[0])\n",
    "\n",
    "mySeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362ef3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the 'shell' dataframe (without actual data)\n",
    "series_nodata = mySeries.drop(['Temperature', 'Irradiation','MPPTdata','IVdataFor','IVdataRev'], axis=1)  # df.columns is zero-based pd.Index\n",
    "series_nodata.to_csv('dataset/pkl_complete/20221109_series_nodata.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489f5c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data as pickle\n",
    "mySeries.to_pickle('dataset/pkl_complete/20221109_selected_pixel.pkl')\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f4efbf",
   "metadata": {},
   "source": [
    "### 1.4. Further pre-processing: cut the dataset to MPPT_EFF, resample for 10 minutes, drop the length to 150 hours, and interpolate if there is NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe8ee04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .pkl file consisting mySeries\n",
    "with open('dataset/pkl_complete/20221109_selected_pixel.pkl', \"rb\") as fh:\n",
    "    mySeries = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420fd5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Column name: ',mySeries.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623c273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mySeries['Irradiation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46d5786",
   "metadata": {},
   "outputs": [],
   "source": [
    "mySeries['Temperature'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a923bbc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combination: convert to datetime + resample and cut down to a certain length\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Defined length to drop (to 150 hours)\n",
    "len_drop = hour_limit*6+1\n",
    "\n",
    "# Going through the data for each one, and resample\n",
    "for i in range(len(mySeries)): # range(len(mySeries)):\n",
    "    print(i)\n",
    "\n",
    "    ################### MPPT ###################\n",
    "    \n",
    "    # Cut the series to MPPT_t and MPPT_EFF\n",
    "    mySeries['MPPTdata'][i] = mySeries['MPPTdata'][i].loc[:,['MPPT_t','MPPT_EFF']]\n",
    "        \n",
    "    # Resample\n",
    "    mySeries['MPPTdata'][i]['MPPT_t'] = pd.to_datetime(mySeries['MPPTdata'][i]['MPPT_t'])\n",
    "    mySeries['MPPTdata'][i] = mySeries['MPPTdata'][i].set_index('MPPT_t').resample('10min').mean() # Every 10 minutes \n",
    "    \n",
    "    # Drop the length\n",
    "    if len(mySeries['MPPTdata'][i]) > len_drop:\n",
    "        mySeries['MPPTdata'][i] = mySeries['MPPTdata'][i].iloc[1:len_drop,:] #Drop first row/ NaN as well\n",
    "    else:\n",
    "        mySeries.drop([i],inplace=True)\n",
    "        \n",
    "    # Interpolate if there's a NaN\n",
    "    if mySeries['MPPTdata'][i].isnull().values.any() == True:\n",
    "        print(i,' MPPTdata has NaN')\n",
    "        mySeries['MPPTdata'][i] = mySeries['MPPTdata'][i].interpolate(method='akima')\n",
    "        \n",
    "    \n",
    "    ################### Temperature ###################\n",
    "    \n",
    "    # Cut the series to MPPT_t and MPPT_EFF\n",
    "    mySeries['Temperature'][i] = mySeries['Temperature'][i].loc[:,['Temperature_t','Temperature']]\n",
    "        \n",
    "    # Resample\n",
    "    mySeries['Temperature'][i]['Temperature_t'] = pd.to_datetime(mySeries['Temperature'][i]['Temperature_t'])\n",
    "    mySeries['Temperature'][i] = mySeries['Temperature'][i].set_index('Temperature_t').resample('10min').mean() # Every 10 minutes \n",
    "    \n",
    "    # Drop the length\n",
    "    if len(mySeries['Temperature'][i]) > len_drop:\n",
    "        mySeries['Temperature'][i] = mySeries['Temperature'][i].iloc[1:len_drop,:] #Drop first row/ NaN as well\n",
    "    else:\n",
    "        mySeries.drop([i],inplace=True)\n",
    "        \n",
    "    # Interpolate if there's a NaN\n",
    "    if mySeries['Temperature'][i].isnull().values.any() == True:\n",
    "        print(i,' temperature has NaN')\n",
    "        mySeries['Temperature'][i] = mySeries['Temperature'][i].interpolate(method='akima')\n",
    "    \n",
    "    ################### Irradiation ###################\n",
    "    \n",
    "    # Cut the series to MPPT_t and MPPT_EFF\n",
    "    mySeries['Irradiation'][i] = mySeries['Irradiation'][i].loc[:,['Irradiation_t','Irradiation']]\n",
    "        \n",
    "    # Resample\n",
    "    mySeries['Irradiation'][i]['Irradiation_t'] = pd.to_datetime(mySeries['Irradiation'][i]['Irradiation_t'])\n",
    "    mySeries['Irradiation'][i] = mySeries['Irradiation'][i].set_index('Irradiation_t').resample('10min').mean() # Every 10 minutes \n",
    "    \n",
    "    # Drop the length\n",
    "    if len(mySeries['Irradiation'][i]) > len_drop:\n",
    "        mySeries['Irradiation'][i] = mySeries['Irradiation'][i].iloc[1:len_drop,:] #Drop first row/ NaN as well\n",
    "    else:\n",
    "        mySeries.drop([i],inplace=True)\n",
    "        \n",
    "    # Interpolate if there's a NaN\n",
    "    if mySeries['Irradiation'][i].isnull().values.any() == True:\n",
    "        print(i,' irradiation has NaN')\n",
    "        mySeries['Irradiation'][i] = mySeries['Irradiation'][i].interpolate(method='akima')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb463ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if it still has NaN\n",
    "\n",
    "for i in range(len(mySeries)):\n",
    "    if mySeries['MPPTdata'][i].isnull().values.any() == True:\n",
    "        print(i,' has NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa4045d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Re-calculate T_avg\n",
    "mySeries['T_avg']=0.0\n",
    "mySeries['Irr_avg']=0.0\n",
    "\n",
    "for num in range(len(mySeries)):\n",
    "    # Add column of average irradiation and temperature\n",
    "    mySeries['T_avg'][num] = mySeries['Temperature'][num]['Temperature'].mean()\n",
    "    mySeries['Irr_avg'][num] = mySeries['Irradiation'][num]['Irradiation'].mean()\n",
    "\n",
    "mySeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78800c2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Trying to plot\n",
    "fig, axs = plt.subplots(10,10,figsize=(35,35))\n",
    "# fig.suptitle('Series')\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i*10+j+1>len(mySeries): # pass the others that we can't fill\n",
    "            continue\n",
    "        axs[i, j].plot(mySeries['MPPTdata'][i*10+j]['MPPT_EFF'])#.values)\n",
    "        # axs[i, j].set_title(namesofMySeries[i*4+j])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2fe069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving as csv\n",
    "mySeries.to_csv('./dataset/pkl_complete/20221109_mySeries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ff73f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving as pickle\n",
    "mySeries.to_pickle('./dataset/pkl_complete/20221109_mySeries.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae7e62",
   "metadata": {},
   "source": [
    "## 2. Load from another jupyter notebook & do further pre-processing\n",
    "\n",
    "**input**: whole .pkl of good pixels\n",
    "\n",
    "**process**:\n",
    "\n",
    "1. check: if there is NaN & length of the data, making sure it's uniform\n",
    "2. group the data into max. PCE group (< 8%, 8-12%, 12-16%, > 16%)\n",
    "3. calculate the relative change of max. PCE group after 150 hours\n",
    "4. plot the data\n",
    "5. scaling/normalization (selected method: MaxAbsScaler)\n",
    "6. smoothing (selected method: savgol/ savitzky-golay_\n",
    "\n",
    "**output**: plots, pre-cleaned data ready for SOM\n",
    "\n",
    "### 2.1. Checking the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce6f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .pkl file consisting the whole dataset\n",
    "with open('dataset/pkl_complete/20221109_mySeries.pkl', \"rb\") as fh:\n",
    "    mySeries = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16159ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the other columns, and only focus on MPPTdata\n",
    "mySeriesDrop = mySeries['MPPTdata']\n",
    "print('Initial length: ',len(mySeriesDrop))\n",
    "\n",
    "# Reindexing for the mySeriesDrop\n",
    "mySeriesDrop.index = range(mySeriesDrop.shape[0])\n",
    "\n",
    "# Checking random row\n",
    "mySeriesDrop[155]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd813f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING LENGTH\n",
    "# Preprocessing, because the data needs to be uniform in length\n",
    "series_lengths = {len(series) for series in mySeriesDrop}\n",
    "\n",
    "# Finding the longest series to elongate the series\n",
    "max_len = max(series_lengths)\n",
    "longest_series = None\n",
    "for series in mySeriesDrop:\n",
    "    if len(series) == max_len:\n",
    "        longest_series = series\n",
    "print('max length in the series: ',max_len)\n",
    "\n",
    "# Finding the shortest series\n",
    "short_name = None\n",
    "min_len = min(series_lengths)\n",
    "shortest_series = None\n",
    "for series in mySeriesDrop:\n",
    "    if len(series) == min_len:\n",
    "        shortest_series = series\n",
    "#         print(series)\n",
    "print('min length in the series: ',min_len)\n",
    "\n",
    "# Looking at the distribution of series lengths\n",
    "# print(type(series_lengths))\n",
    "sns.set_style('darkgrid')\n",
    "sns.distplot(np.array(list(series_lengths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c09127",
   "metadata": {},
   "source": [
    "### 2.2. Now, let's plot the data\n",
    "\n",
    "1. Calculate the relative change in max. PCE (after 150 hours). \n",
    "\n",
    "Relative change in max. PCE = (PCE_max-PCE_150h)/(PCE_max)\n",
    "For calculating these PCE values: we take the mean for 3 or 5 points around the values of interest, making sure that it's not considering the noise only.\n",
    "\n",
    "2. Group the max. PCE\n",
    "\n",
    "3. Plot the following figures:\n",
    "    - Max. PCE group vs. relative change in max. PCE (after 150 hours)\n",
    "    - Each max. PCE group's degradation traces over time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce4cddb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PCEbefore_list = []\n",
    "PCEafter_list = []\n",
    "PCEdelta_list = []\n",
    "\n",
    "for i in range(len(mySeriesDrop)):\n",
    "    \n",
    "    ### Calculate the relative change\n",
    "    \n",
    "    ## Extracting initial PCE value\n",
    "    PCEbefore_1 = mySeriesDrop[i].iloc[0]['MPPT_EFF'] # only the initial\n",
    "    PCEbefore_2 = mySeriesDrop[i]['MPPT_EFF'].head(3).mean() # take the mean\n",
    "    \n",
    "    ## Extracting final PCE value at 150 h\n",
    "    PCEafter_1 = mySeriesDrop[i].iloc[899]['MPPT_EFF'] # only the tail\n",
    "    PCEafter_2 = mySeriesDrop[i]['MPPT_EFF'].tail(3).mean() # take the mean\n",
    "    \n",
    "    ## Extracting maximum PCE\n",
    "    \n",
    "    # 1st: just extract top 5\n",
    "    # PCEtopbef_1 = (mySeriesDrop[i].sort_values(by=['MPPT_EFF']).head(5)).mean() #top 5\n",
    "    \n",
    "    # 2nd: extract top 3\n",
    "    PCEtopbef_1 = (mySeriesDrop[i].nlargest(3,['MPPT_EFF'])).mean().item() # top 5 \n",
    "    \n",
    "    # 3rd: find location of top PCE, and averaging around it\n",
    "    # location_max_PCE = mySeriesDrop[i]['MPPT_EFF'].idxmax()\n",
    "    # iloc_max_PCE = (mySeriesDrop[i]['MPPT_EFF']).index.get_loc(location_max_PCE)\n",
    "\n",
    "    # if iloc_max_PCE >= 888: # See if it's at the tail or head, and make adjustments\n",
    "    #     selected_rows = mySeriesDrop[i]['MPPT_EFF'].iloc[[iloc_max_PCE-2, iloc_max_PCE-1, iloc_max_PCE]]\n",
    "    # elif iloc_max_PCE <= 1:\n",
    "    #     selected_rows = mySeriesDrop[i]['MPPT_EFF'].iloc[[iloc_max_PCE, iloc_max_PCE+1, iloc_max_PCE+2]]\n",
    "    # else:\n",
    "    #     selected_rows = mySeriesDrop[i]['MPPT_EFF'].iloc[[iloc_max_PCE-2, iloc_max_PCE-1, iloc_max_PCE,   \n",
    "    #                                                       iloc_max_PCE+1, iloc_max_PCE+2]]\n",
    "        \n",
    "    # PCEtopbef_1 = selected_rows.mean()\n",
    "    \n",
    "    # Print the results\n",
    "    # print('row ',i,' iloc ', iloc_max_PCE, ' location of max PCE: ',location_max_PCE,\n",
    "    #       ' top mean: ', PCEtopbef_1)\n",
    "    \n",
    "    ## Calculate the relative change\n",
    "    PCEdelta = (PCEtopbef_1-PCEafter_1)*100/PCEtopbef_1 # top efficiency\n",
    "    \n",
    "    PCEbefore_list.append(PCEtopbef_1)\n",
    "    PCEafter_list.append(PCEafter_1)\n",
    "    PCEdelta_list.append(PCEdelta)\n",
    "\n",
    "# Combine the results to put in the dataframe\n",
    "PCE_combined = [PCEbefore_list, PCEafter_list, PCEdelta_list]\n",
    "PCE_combined_transposed = np.array(PCE_combined).T.tolist()\n",
    "PCE_df = pd.DataFrame (PCE_combined_transposed, columns = ['PCE_before', 'PCE_after','PCE_delta'])\n",
    "\n",
    "# Load libraries for plotting\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Plot the overview\n",
    "fig = go.Figure(data=go.Scatter(x=PCEbefore_list, y=PCEdelta_list, mode='markers'))\n",
    "fig.update_layout(yaxis=dict(range=[-2.1,2.2]),xaxis_title=\"Initial PCE (%)\",yaxis_title=\"Relative hange in max. PCE (after 150 hrs.) (%)\")\n",
    "fig.update_yaxes(type='log')\n",
    "\n",
    "# To display the figure in the output screen\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dc2740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by PCE_before\n",
    "PCE_df = PCE_df.sort_values(by=['PCE_before'])\n",
    "PCE_df['PCE_delta'].iloc[209:466]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3073b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many groups do we want?\n",
    "n_group = 5 \n",
    "lengthdf = len(PCE_df)\n",
    "lengthpergroup = np.round(lengthdf/n_group)\n",
    "last_n = 0\n",
    "\n",
    "# Sort by PCE_before\n",
    "PCE_df = PCE_df.sort_values(by=['PCE_before'])\n",
    "\n",
    "# Grouping the PCE_before (top) into n groups (dummy, will be replaced with other values)\n",
    "PCE_df['PCE_before_ceil'] = PCE_df['PCE_before'].apply(np.ceil)\n",
    "PCE_df['PCE_before_x'] = PCE_df['PCE_before'].apply(np.ceil)\n",
    "PCE_df['PCE_before_ceil_x'] = PCE_df['PCE_before'].apply(np.ceil)\n",
    "PCE_df['PCE_before_median_x'] = PCE_df['PCE_before'].apply(np.ceil)\n",
    "PCE_df['PCE_before_mean_x'] = PCE_df['PCE_before'].apply(np.ceil)\n",
    "\n",
    "# Loop for all the groups\n",
    "for i in range(n_group):\n",
    "    if i!= n_group-1:\n",
    "        PCE_df['PCE_before_x'].iloc[int(last_n):int((i+1)*lengthpergroup)] = i+1\n",
    "        PCE_df['PCE_before_ceil_x'].iloc[int(last_n):int((i+1)*lengthpergroup)] = PCE_df['PCE_before'].iloc[int((i+1)*lengthpergroup)]\n",
    "        PCE_df['PCE_before_median_x'].iloc[int(last_n):int((i+1)*lengthpergroup)] = PCE_df['PCE_before'].loc[PCE_df['PCE_before_x'] == (i+1)].median()\n",
    "        PCE_df['PCE_before_mean_x'].iloc[int(last_n):int((i+1)*lengthpergroup)] = PCE_df['PCE_before'].loc[PCE_df['PCE_before_x'] == (i+1)].mean()\n",
    "        last_n = (i+1)*lengthpergroup\n",
    "    else:\n",
    "        PCE_df['PCE_before_x'].iloc[int(last_n):] = i+1\n",
    "        PCE_df['PCE_before_ceil_x'].iloc[int(last_n):] = PCE_df['PCE_before'].iloc[-1]\n",
    "        PCE_df['PCE_before_median_x'].iloc[int(last_n):] = PCE_df['PCE_before'].loc[PCE_df['PCE_before_x'] == (i+1)].median()\n",
    "        PCE_df['PCE_before_mean_x'].iloc[int(last_n):] = PCE_df['PCE_before'].loc[PCE_df['PCE_before_x'] == (i+1)].mean()\n",
    "\n",
    "# Print unique values for each group\n",
    "unique_ceil = PCE_df['PCE_before_ceil_x'].unique()\n",
    "unique_median = PCE_df['PCE_before_median_x'].unique()\n",
    "unique_mean = PCE_df['PCE_before_mean_x'].unique()\n",
    "\n",
    "# Sort by PCE_before_ceil_x\n",
    "PCE_df = PCE_df.sort_values(by=['PCE_before_ceil_x'])\n",
    "\n",
    "# Print length of PCE_before_ceil_x\n",
    "for i in unique_ceil:\n",
    "    print(\"{:.1f}\".format(i), ' ceil: ', len(PCE_df[PCE_df['PCE_before_ceil_x']==i]))\n",
    "\n",
    "# Save as csv\n",
    "PCE_df.to_csv(filedirname+'PCE_df_grouping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb89b7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot boxplot\n",
    "fig = px.box(PCE_df, x=\"PCE_before_x\", y=\"PCE_delta\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103fe761",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot violin plot\n",
    "fig = px.violin(PCE_df, x=\"PCE_before_x\", y=\"PCE_delta\", \n",
    "                box=True, points=\"all\",hover_data=PCE_df.columns)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cc3424",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "import colorlover as cl\n",
    "from plotly.colors import n_colors\n",
    "import matplotlib\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "a = ['PCE < 10%','PCE 10-14%','PCE 14-16.6%','PCE 16.6-19.2%', 'PCE > 19.2%']\n",
    "\n",
    "print('Unique ceil: ',unique_ceil)\n",
    "print('Median: ',unique_median)\n",
    "print('Mean: ',unique_mean)\n",
    "\n",
    "# Color palette for the figure to make it pretty\n",
    "colors = n_colors('rgb(8,29,88)', 'rgb(127,205,187)', 5, colortype='rgb')\n",
    "colors_box = n_colors('rgb(2,7,22)', 'rgb(30,50,45)', 5, colortype='rgb')\n",
    "colors_line = n_colors('rgb(0,5,15)', 'rgb(15,25,23)', 5, colortype='rgb')\n",
    "\n",
    "# Plotting the violin and boxplot\n",
    "for (i,color,color_line) in zip(unique_ceil, colors, colors_line):\n",
    "    fig.add_trace(go.Violin(x=PCE_df['PCE_before_x'][PCE_df['PCE_before_ceil_x'] == i],\n",
    "                            y=PCE_df['PCE_delta'][PCE_df['PCE_before_ceil_x'] == i],\n",
    "                            box_visible=False,\n",
    "                            fillcolor = color,\n",
    "                            opacity = 0.4,\n",
    "                            line = dict(color=color_line),\n",
    "                            jitter=True,\n",
    "                            meanline_visible=False))\n",
    "\n",
    "for (i,color,color_line) in zip(unique_ceil, colors, colors_line):\n",
    "    fig.add_trace(go.Box(x=PCE_df['PCE_before_x'][PCE_df['PCE_before_ceil_x'] == i],\n",
    "                            y=PCE_df['PCE_delta'][PCE_df['PCE_before_ceil_x'] == i],\n",
    "                            marker_color = color,\n",
    "                            opacity = 0.55,\n",
    "                            line_color = color_line,\n",
    "                            fillcolor = color,\n",
    "                            jitter=True,\n",
    "                            boxmean=True))\n",
    "\n",
    "fig.update_layout(xaxis_title=\"Max. PCE group (%)\",\n",
    "                  yaxis_title=\"Relative change in max. PCE (after 150 hrs.) (%)\",\n",
    "                  boxgap = 0.85,\n",
    "                  font_family='Arial',\n",
    "                  showlegend=False)\n",
    "    \n",
    "fig.show()\n",
    "\n",
    "# Save the figure \n",
    "pio.write_image(fig, filedirname+'all_data_changedegradation_4.png',\n",
    "                width=900, height=600, scale=22)\n",
    "\n",
    "pio.write_image(fig, filedirname+'all_data_changedegradation_3.png',\n",
    "                width=700, height=450, scale=25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d46fe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting specific group and plotting their general degradation trends\n",
    "\n",
    "import random\n",
    "\n",
    "x = np.linspace(0,150, 900, endpoint=True)\n",
    "x = np.linspace(0,500, 3000, endpoint=True)\n",
    "k = 1\n",
    "\n",
    "index_1 = PCE_df.index[PCE_df['PCE_before_x'] == 1].tolist()\n",
    "index_2 = PCE_df.index[PCE_df['PCE_before_x'] == 2].tolist()\n",
    "index_3 = PCE_df.index[PCE_df['PCE_before_x'] == 3].tolist()\n",
    "index_4 = PCE_df.index[PCE_df['PCE_before_x'] == 4].tolist()\n",
    "index_5 = PCE_df.index[PCE_df['PCE_before_x'] == 5].tolist()\n",
    "\n",
    "mySeries_selected_1 = mySeriesDrop.iloc[index_1] # all\n",
    "mySeries_selected_2 = mySeriesDrop.iloc[index_2] # all\n",
    "mySeries_selected_3 = mySeriesDrop.iloc[index_3] # all\n",
    "mySeries_selected_4 = mySeriesDrop.iloc[index_4] # all\n",
    "mySeries_selected_5 = mySeriesDrop.iloc[index_5] # all\n",
    "\n",
    "# Randomly selected\n",
    "# mySeries_selected_8 = mySeries_selected.iloc[random.choices(index_8, k=k)] #random selection\n",
    "# mySeries_selected_12 = mySeries_selected.iloc[random.choices(index_12, k=k)] #random selection\n",
    "# mySeries_selected_16 = mySeries_selected.iloc[random.choices(index_16, k=k)] #random selection\n",
    "# mySeries_selected_20 = mySeries_selected.iloc[random.choices(index_20, k=k)] #random selection\n",
    "\n",
    "# Function to convert the series to df for specific, selected indexes\n",
    "def convert_to_df (series, index_specified):\n",
    "    big_df = pd.DataFrame()\n",
    "    for i in index_specified:\n",
    "        l = series[i].reset_index()\n",
    "        big_df = pd.concat([big_df, l['MPPT_EFF']], axis='columns')\n",
    "    \n",
    "    # Calculate statistics for this new df\n",
    "    big_df['median'] = big_df.median(axis=1)\n",
    "    big_df['mean'] = big_df.mean(axis=1)\n",
    "    big_df['lower_quartile'] = big_df.quantile(0.25, axis=1)\n",
    "    big_df['upper_quartile'] = big_df.quantile(0.75, axis=1)\n",
    "    big_df['lower_quartile_graph'] = big_df['lower_quartile'][::-1]\n",
    "    \n",
    "    return big_df[['mean', 'median', 'lower_quartile', 'upper_quartile',\n",
    "                   'lower_quartile_graph']]\n",
    "        \n",
    "stat_1 = convert_to_df(mySeries_selected_1, index_1)\n",
    "stat_2 = convert_to_df(mySeries_selected_2, index_2)\n",
    "stat_3 = convert_to_df(mySeries_selected_3, index_3)\n",
    "stat_4 = convert_to_df(mySeries_selected_4, index_4)\n",
    "stat_5 = convert_to_df(mySeries_selected_5, index_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7407132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for degradation traces for specific max. PCE group\n",
    "\n",
    "x_rev = x[::-1]\n",
    "\n",
    "colors = n_colors('rgb(8,29,88)', 'rgb(127,205,187)', 5, colortype='rgb')\n",
    "fill_colors = n_colors('rgba(8,29,88,0.2)', 'rgb(127,205,187,0.2)', 5, colortype='rgb')\n",
    "\n",
    "a = ['PCE < 10%','PCE 10-14%','PCE 14-16.6%','PCE 16.6-19.2%', 'PCE > 19.2%']\n",
    "b = ['< 10%','10-14%','14-16.6%','16.6-19.2%', '> 19.2%']\n",
    "\n",
    "a = ['PCE < 10.6%','PCE 10.6-14.5%','PCE 14.5-17%','PCE 17-19.6%', 'PCE > 19.6%']\n",
    "b = ['< 10.6%','10.6-14.5%','14.5-17%','17-19.6%', '> 19.6%']\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Group 1\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x,\n",
    "    y=stat_1['upper_quartile'],\n",
    "    fill=None,\n",
    "#     fillcolor='rgba(0,100,80,0.2)',\n",
    "    line_color='rgba(255,255,255,0)',\n",
    "    mode='lines',\n",
    "    showlegend=False,\n",
    "    name=b[0],\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x,\n",
    "    y=stat_1['lower_quartile'],\n",
    "    fill='tonexty',\n",
    "    mode='lines',\n",
    "    fillcolor='rgba(8,29,88,0.2)',\n",
    "    line_color='rgba(255,255,255,0)',\n",
    "    showlegend=False,\n",
    "    name=b[0],\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x, y=stat_1['median'],\n",
    "    line_color= colors[0],#'rgb(0,100,80)',\n",
    "    name=b[0],\n",
    "))\n",
    "\n",
    "# Group 2\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x,\n",
    "    y=stat_2['upper_quartile'],\n",
    "    fill=None,\n",
    "#     fillcolor='rgba(0,100,80,0.2)',\n",
    "    line_color='rgba(255,255,255,0)',\n",
    "    mode='lines',\n",
    "    showlegend=False,\n",
    "    name=b[1],\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x,\n",
    "    y=stat_2['lower_quartile'],\n",
    "    fill='tonexty',\n",
    "    mode='lines',\n",
    "    fillcolor='rgba(31.8, 64.2, 107.8,0.2)',\n",
    "    line_color='rgba(255,255,255,0)',\n",
    "    showlegend=False,\n",
    "    name=b[1],\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x, y=stat_2['median'],\n",
    "    line_color= colors[1],#'rgb(0,100,80)',\n",
    "    name=b[1],\n",
    "))\n",
    "\n",
    "# Group 3\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x,\n",
    "    y=stat_3['upper_quartile'],\n",
    "    fill=None,\n",
    "#     fillcolor='rgba(0,100,80,0.2)',\n",
    "    line_color='rgba(255,255,255,0)',\n",
    "    mode='lines',\n",
    "    showlegend=False,\n",
    "    name=b[2],\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x,\n",
    "    y=stat_3['lower_quartile'],\n",
    "    fill='tonexty',\n",
    "    mode='lines',\n",
    "    fillcolor='rgba(55.6, 99.4, 127.6,0.2)',\n",
    "    line_color='rgba(255,255,255,0)',\n",
    "    showlegend=False,\n",
    "    name=b[2],\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x, y=stat_3['median'],\n",
    "    line_color= colors[2],#'rgb(0,100,80)',\n",
    "    name=b[2],\n",
    "))\n",
    "\n",
    "# Group 4\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x,\n",
    "    y=stat_4['upper_quartile'],\n",
    "    fill=None,\n",
    "#     fillcolor='rgba(0,100,80,0.2)',\n",
    "    line_color='rgba(255,255,255,0)',\n",
    "    mode='lines',\n",
    "    showlegend=False,\n",
    "    name=b[3],\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x,\n",
    "    y=stat_4['lower_quartile'],\n",
    "    fill='tonexty',\n",
    "    mode='lines',\n",
    "    fillcolor='rgba(79.4, 134.60000000000002, 147.4,0.2)',\n",
    "    line_color='rgba(255,255,255,0)',\n",
    "    showlegend=False,\n",
    "    name=b[3],\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x, y=stat_4['median'],\n",
    "    line_color= colors[3],#'rgb(0,100,80)',\n",
    "    name=b[3],\n",
    "))\n",
    "\n",
    "# Group 5\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x,\n",
    "    y=stat_5['upper_quartile'],\n",
    "    fill=None,\n",
    "#     fillcolor='rgba(0,100,80,0.2)',\n",
    "    line_color='rgba(255,255,255,0)',\n",
    "    mode='lines',\n",
    "    showlegend=False,\n",
    "    name=b[4],\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x,\n",
    "    y=stat_5['lower_quartile'],\n",
    "    fill='tonexty',\n",
    "    mode='lines',\n",
    "    fillcolor='rgba(103.2, 169.8, 167.2,0.2)',\n",
    "    line_color='rgba(255,255,255,0)',\n",
    "    showlegend=False,\n",
    "    name=b[4],\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x, y=stat_5['median'],\n",
    "    line_color= colors[4],#'rgb(0,100,80)',\n",
    "    name=b[4],\n",
    "))\n",
    "\n",
    "fig.update_traces(mode='lines')\n",
    "\n",
    "# Save a figure\n",
    "pio.write_image(fig, filedirname+'stat_all_1.png', width=1*600, height=600, scale=12)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbab4237",
   "metadata": {},
   "source": [
    "#### Now, looking at different layers that the devices have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a3abf2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Look at the unique types for each column\n",
    "\n",
    "print('BackContact: ',mySeries['BackContact'].unique())\n",
    "print('FrontContact: ',mySeries['FrontContact'].unique())\n",
    "print('CSL1: ',mySeries['CSL1'].unique())\n",
    "print('CSL2: ',mySeries['CSL2'].unique())\n",
    "print('CSL3: ',mySeries['CSL3'].unique())\n",
    "print('CSL4: ',mySeries['CSL4'].unique())\n",
    "\n",
    "print('Column name: ',mySeries.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99794025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the specific entries and their values\n",
    "\n",
    "mySeries['T_avg'].value_counts()\n",
    "# Plotting the distribution of temperature\n",
    "sns.set_style('darkgrid')\n",
    "sns.set(rc={'figure.figsize':(5,4)})\n",
    "sns.distplot(mySeries['T_avg'],kde=False)\n",
    "\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.xlabel('Average degradation temperature (deg. C)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(filedirname+'distribution_temperature.png', dpi=600)\n",
    "\n",
    "# Print the min and max hour\n",
    "print('min temp: ',min(mySeries['T_avg']))\n",
    "print('max temp: ',max(mySeries['T_avg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ec6138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the specific entries and their values\n",
    "\n",
    "mySeries['Irr_avg'].value_counts()\n",
    "# Plotting the distribution of temperature\n",
    "sns.set_style('darkgrid')\n",
    "sns.set(rc={'figure.figsize':(5,4)})\n",
    "sns.distplot(mySeries['Irr_avg'],kde=False)\n",
    "\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.xlabel('Average degradation irradiation (W/m2)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(filedirname+'distribution_irradiation.png', dpi=600)\n",
    "\n",
    "# Print the min and max hour\n",
    "print('min irradiation: ',min(mySeries['T_avg']))\n",
    "print('max irradiation: ',max(mySeries['T_avg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6dc137",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at the specific entries and their values\n",
    "\n",
    "mySeries['FrontContact'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8fed03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at the specific entries and their values\n",
    "\n",
    "mySeries['BackContact'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88f008a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Look at the specific entries and their values\n",
    "\n",
    "mySeries['CSL1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c4021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the specific entries and their values\n",
    "\n",
    "mySeries['CSL2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacb8c8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at the specific entries and their values\n",
    "\n",
    "mySeries['ABS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3620bf16",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Look at the specific entries and their values\n",
    "\n",
    "mySeries['CSL3'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f7234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the specific entries and their values\n",
    "\n",
    "mySeries['CSL4'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad8a1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the mySeriesDrop as .pkl file (only has MPPTdata)\n",
    "mySeriesDrop.to_pickle('./dataset/pkl_complete/20221109_mySeriesDrop.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0f176a",
   "metadata": {},
   "source": [
    "### 2.3. Scaling/ normalization\n",
    "\n",
    "There are two types of scaling/ normalization:\n",
    "\n",
    "1. sklearn.preprocessing.MinMaxScaler -> scaling between min-max of the data (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler)\n",
    "\n",
    "2. sklearn.preprocessing.MaxAbsScaler -> scaling between 0 and max of the data (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html#sklearn.preprocessing.MaxAbsScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37613e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mySeriesDrop that only has MPPTdata\n",
    "with open('dataset/pkl_complete/20221109_mySeriesDrop.pkl', \"rb\") as fh:\n",
    "    mySeriesDrop = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049efec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: scaling/ normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "# Function to scale, normalize and plot it\n",
    "def normalize(mySeriesDrop,normalizationMethod):\n",
    "    '''\n",
    "    A function to normalize and plot the result\n",
    "    \n",
    "    input:\n",
    "    1. mySeriesDrop (only contains MPPTdata)\n",
    "    2. normalizationMethod: a string of normalization type, 'MinMaxScaler',\n",
    "       'MaxAbsScaler'\n",
    "    \n",
    "    '''\n",
    "    mySeriesDrop_norm = mySeriesDrop.copy()\n",
    "    \n",
    "    # MinMaxScaler\n",
    "    if normalizationMethod == 'MinMaxScaler':\n",
    "        for i in range(len(mySeriesDrop_norm)):\n",
    "            scaler = MinMaxScaler()\n",
    "            mySeriesDrop_norm[i] = MinMaxScaler().fit_transform(mySeriesDrop_norm[i])\n",
    "            mySeriesDrop_norm[i]= mySeriesDrop_norm[i].reshape(len(mySeriesDrop_norm[i]))\n",
    "    \n",
    "    # MaxAbsScaler\n",
    "    elif normalizationMethod == 'MaxAbsScaler':\n",
    "        for i in range(len(mySeriesDrop_norm)):\n",
    "            scaler = MaxAbsScaler()\n",
    "            mySeriesDrop_norm[i] = MaxAbsScaler().fit_transform(mySeriesDrop_norm[i])\n",
    "            mySeriesDrop_norm[i]= mySeriesDrop_norm[i].reshape(len(mySeriesDrop_norm[i]))\n",
    "    \n",
    "    # Plot the first 100 of data\n",
    "    fig, axs = plt.subplots(10,10,figsize=(30,30), sharex=True, sharey=True)\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            if i*10+j+1>len(mySeriesDrop_norm): # pass the others that we can't fill\n",
    "                continue\n",
    "            axs[i, j].plot(mySeriesDrop_norm[i*10+j])\n",
    "\n",
    "    plt.ylim([0,1])\n",
    "    plt.show()\n",
    "    \n",
    "    return mySeriesDrop_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ef4b58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mySeriesDrop_maxAbs = normalize(mySeriesDrop,'MaxAbsScaler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec72994",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# mySeriesDrop_minMax = normalize(mySeriesDrop,'MinMaxScaler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05cf8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the mySeriesDrop as .pkl file (only has MPPTdata)\n",
    "mySeriesDrop_maxAbs.to_pickle('./dataset/pkl_complete/20221109_mySeriesDropNorm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1c4173",
   "metadata": {},
   "source": [
    "### 2.4. Smoothing \n",
    "\n",
    "Because the data is noisy, we are going to do some 'smoothing' using Savitzky-Golay filter (https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.savgol_filter.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d799b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mySeriesDrop that only has MPPTdata\n",
    "with open('dataset/pkl_complete/20221109_mySeriesDropNorm.pkl', \"rb\") as fh:\n",
    "    mySeriesDrop = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a2242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mySeriesDrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c043bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting the overview of the MPPT data (the first 100 data)\n",
    "\n",
    "fig, axs = plt.subplots(10,10,figsize=(30,30),sharex=True,sharey=True)\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i*10+j+1>len(mySeriesDrop): # pass the others that we can't fill\n",
    "            continue\n",
    "        axs[i, j].plot(mySeriesDrop[i*10+j])\n",
    "        \n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b2064c",
   "metadata": {},
   "source": [
    "We are showing 3 different methods for smoothing:\n",
    "\n",
    "1. np.convolve (rolling average)\n",
    "\n",
    "2. scipy.signal.lfilter\n",
    "\n",
    "3. scipy.signal.savgol (savitzky-golay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75091e8f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert to rolling average/ np.convolve\n",
    "\n",
    "def smooth(y, box_pts):\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    return y_smooth\n",
    "\n",
    "curve_interest = mySeriesDrop[57]\n",
    "\n",
    "# Plotting the result\n",
    "y_smooth_3 = smooth(curve_interest,3)\n",
    "plt.figure(figsize=(4,2),dpi=300)\n",
    "plt.plot(curve_interest,'o',alpha=0.05)\n",
    "plt.plot(smooth(curve_interest,3), 'r-', lw=1)\n",
    "plt.plot(smooth(curve_interest,50), 'g-', lw=1)\n",
    "\n",
    "plt.legend(['actual data','convolve:3','convolve:50'])\n",
    "\n",
    "print(len(curve_interest), len(y_smooth_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3812e8f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Smoothing using l filter\n",
    "\n",
    "from scipy.signal import lfilter\n",
    "\n",
    "n1 = 15  # the larger n is, the smoother curve will be\n",
    "b1 = [1.0 / n1] * n1\n",
    "a1 = 1\n",
    "\n",
    "n2 = 30  # the larger n is, the smoother curve will be\n",
    "b2 = [1.0 / n2] * n2\n",
    "a2 = 1\n",
    "\n",
    "curve_interest = mySeriesDrop[57]\n",
    "\n",
    "# Plotting with l filter\n",
    "y_smooth_3 = smooth(curve_interest,3)\n",
    "plt.figure(figsize=(4,2),dpi=300)\n",
    "plt.plot(curve_interest,'o',alpha=0.05)\n",
    "plt.plot(lfilter(b1,a1,curve_interest), 'r-', lw=1)\n",
    "plt.plot(lfilter(b2,a2,curve_interest), 'g-', lw=1)\n",
    "\n",
    "plt.legend(['actual data','lfilter n:15','lfilter n:30'])\n",
    "\n",
    "print(len(curve_interest), len(lfilter(b1,a1,curve_interest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d40997",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using savitzky-golay filter\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "curve_interest = mySeriesDrop[57]\n",
    "\n",
    "w1 = savgol_filter(curve_interest, 71, 2)\n",
    "w2 = savgol_filter(curve_interest, 201, 2)\n",
    "\n",
    "# Plotting the figure\n",
    "plt.figure(figsize=(4,2),dpi=300)\n",
    "plt.plot(curve_interest,'o',alpha=0.05)\n",
    "plt.plot(w1, 'r-', lw=1)\n",
    "plt.plot(w2, 'g-', lw=1)\n",
    "\n",
    "print(len(curve_interest), len(w1), len(w2))\n",
    "\n",
    "plt.legend(['actual data','savgol window:71','savgol window:201'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28feea2a",
   "metadata": {},
   "source": [
    "Since Savgol with parameter=71 seems to work the best at smoothing, we are going to use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7dde46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert to savgol: 71\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "# sns.reset_orig()\n",
    "\n",
    "# mySeriesDrop type is pandas Series, and mySeriesDrop[0] type is numpy array\n",
    "\n",
    "n = 71\n",
    "mySeriesDrop_savgol = []\n",
    "\n",
    "# Calculating savgol series for all the rows\n",
    "for i in range(len(mySeriesDrop)):\n",
    "    savgol = savgol_filter(mySeriesDrop[i], n,2)\n",
    "    mySeriesDrop_savgol.append(savgol)\n",
    "\n",
    "# Trying to plot after savgol filter\n",
    "fig, axs = plt.subplots(7,7,figsize=(18,18),sharex=True)\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "for i in range(7):\n",
    "    for j in range(7):\n",
    "        if i*7+j+1>len(mySeriesDrop_savgol): # pass the others that we can't fill\n",
    "            continue\n",
    "        axs[i, j].plot(mySeriesDrop[i*7+j],'o',color='b',alpha=0.05)#.values)\n",
    "        axs[i, j].plot(mySeriesDrop_savgol[i*7+j],color='r',lw=2)#.values)\n",
    "\n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd24875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save numpy array as .npy instead of .pkl\n",
    "np.save('dataset/pkl_complete/20221109_mySeriesDrop_savgol.npy',mySeriesDrop_savgol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03d0d5e",
   "metadata": {},
   "source": [
    "## 3. SOM/ self-organizing map\n",
    "\n",
    "Read more about SOM here: https://en.wikipedia.org/wiki/Self-organizing_map.\n",
    "\n",
    "**input**: clean, pre-processed MPPT data\n",
    "\n",
    "**process**:\n",
    "\n",
    "1. cluster them using SOM, explore 3 different parameters combination to see how consistent the clustering results are\n",
    "2. plot the clusters and distribution\n",
    "3. split based on the device architecture, plot them\n",
    "4. look at both clusters and max. PCE group (see if certain clusters correlate with certain max. PCE group more)\n",
    "5. trendline of relative change -150hrs and the max. PCE group\n",
    "\n",
    "**output**: \n",
    "1. som clusters\n",
    "2. plots\n",
    "3. trendline (what is the x-intercept?)\n",
    "\n",
    "### 3.1. SOM clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f64c60d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "\n",
    "import math\n",
    "from minisom import MiniSom\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from PIL import ImageColor\n",
    "\n",
    "# Load libraries for plotting\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d28202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing color palettes and opacity\n",
    "\n",
    "opacity = 0.5\n",
    "\n",
    "colors=[ImageColor.getcolor(px.colors.qualitative.Pastel1[0],'RGB'),\n",
    "        ImageColor.getcolor(px.colors.qualitative.Pastel1[1],'RGB'),\n",
    "        ImageColor.getcolor(px.colors.qualitative.Pastel1[2],'RGB'),\n",
    "        ImageColor.getcolor(px.colors.qualitative.Pastel1[3],'RGB')]\n",
    "\n",
    "colors_solid=[ImageColor.getcolor(px.colors.qualitative.Set1[0],'RGB'),\n",
    "              ImageColor.getcolor(px.colors.qualitative.Set1[1],'RGB'),\n",
    "              ImageColor.getcolor(px.colors.qualitative.Set1[2],'RGB'),\n",
    "              ImageColor.getcolor(px.colors.qualitative.Set1[3],'RGB')]\n",
    "\n",
    "colors_rgba=[]\n",
    "colors_solid_rgba=[]\n",
    "\n",
    "for i in range(len(colors)):\n",
    "    colors_rgba.append('rgba'+str(colors[i])[:-1]+','+str(opacity)+')')\n",
    "    \n",
    "for i in range(len(colors_solid)):\n",
    "    colors_solid_rgba.append('rgba'+str(colors_solid[i])[:-1]+','+str(opacity)+')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dd6ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with plotly\n",
    "\n",
    "import plotly.io as pio\n",
    "import colorlover as cl\n",
    "from plotly.colors import n_colors\n",
    "import matplotlib\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Functions to plot the series\n",
    "\n",
    "# Plot using averaged center\n",
    "    \n",
    "def plot_som_series_averaged_center(som_x, som_y, win_map, name):\n",
    "    \n",
    "    fig = make_subplots(\n",
    "    rows=som_x, cols=som_y,\n",
    "    shared_xaxes=True,\n",
    "    shared_yaxes=True,\n",
    "    vertical_spacing=0.1,\n",
    "    )\n",
    "    \n",
    "    # Colors\n",
    "    opacity = 0.04\n",
    "\n",
    "    colors=[ImageColor.getcolor(px.colors.qualitative.Antique[4],'RGB'),\n",
    "            ImageColor.getcolor(px.colors.qualitative.Antique[9],'RGB'),\n",
    "            ImageColor.getcolor(px.colors.qualitative.Antique[6],'RGB'),\n",
    "            ImageColor.getcolor(px.colors.qualitative.Antique[8],'RGB')]\n",
    "\n",
    "    colors_solid=[ImageColor.getcolor(px.colors.qualitative.Set1[0],'RGB'),\n",
    "                  ImageColor.getcolor(px.colors.qualitative.Set1[1],'RGB'),\n",
    "                  ImageColor.getcolor(px.colors.qualitative.Set1[2],'RGB'),\n",
    "                  ImageColor.getcolor(px.colors.qualitative.Set1[3],'RGB')]\n",
    "\n",
    "    colors_rgba=[]\n",
    "    colors_solid_rgba=[]\n",
    "\n",
    "    for i in range(len(colors)):\n",
    "        colors_rgba.append('rgba'+str(colors[i])[:-1]+','+str(opacity)+')')\n",
    "\n",
    "    for i in range(len(colors_solid)):\n",
    "        colors_solid_rgba.append('rgba'+str(colors_solid[i])[:-1]+','+str(opacity)+')')\n",
    "    \n",
    "    # Color count\n",
    "    color_count = 0\n",
    "    \n",
    "    # Time\n",
    "    time = np.linspace(0,150, 900, endpoint=True)\n",
    "    \n",
    "    # Create the subplots\n",
    "    for x in range(som_x):\n",
    "        for y in range(som_y):\n",
    "            cluster = (x,y)\n",
    "            cluster_number = x*som_y+y\n",
    "            if cluster in win_map.keys():\n",
    "\n",
    "                for series in win_map[cluster]:\n",
    "                    \n",
    "                    # Cluster colors\n",
    "                    if cluster==(0,0):\n",
    "                        line_color = colors_rgba[0]\n",
    "                        solid_color = colors_solid_rgba[0]\n",
    "                    elif cluster==(0,1):\n",
    "                        line_color = colors_rgba[1]\n",
    "                        solid_color = colors_solid_rgba[1]\n",
    "                    elif cluster==(1,0):\n",
    "                        line_color = colors_rgba[2]\n",
    "                        solid_color = colors_solid_rgba[2]\n",
    "                    else:\n",
    "                        line_color = colors_rgba[3]\n",
    "                        solid_color = colors_solid_rgba[3]\n",
    "                        \n",
    "                    # Plot the traces \n",
    "                    fig.add_trace(go.Scatter(x=time, y=series, mode='lines',\n",
    "                                             name=f\"Cluster {cluster_number}\",\n",
    "                                             line=dict(color=line_color),\n",
    "                                             showlegend=False),\n",
    "                                  row=x+1, col=y+1)\n",
    "                color_count=+1\n",
    "                \n",
    "                # Calculate the average\n",
    "                cluster_mean= np.average(np.vstack(win_map[cluster]),axis=0)\n",
    "                \n",
    "                # Plot the average\n",
    "                fig.add_trace(go.Scatter(x=time, y= cluster_mean, mode='lines',\n",
    "                                         name=f\"Cluster mean {cluster_number}\",\n",
    "                                         line_color='black',\n",
    "                                         showlegend=False),\n",
    "                              row=x+1, col=y+1)\n",
    "            \n",
    "            # Update the figure\n",
    "            fig.update_yaxes(range=[-0.1,1.1], row=x+1, col=y+1)\n",
    "            fig.update_layout(font_family='Arial')\n",
    "\n",
    "    # Save the figure\n",
    "    pio.write_image(fig, name+'averagedcenter.png', width=1*600, height=600, scale=15)\n",
    "    \n",
    "    # Showing the figure\n",
    "    fig.show()\n",
    "    \n",
    "\n",
    "# Plot using barycenter \n",
    "def plot_som_series_dba_center(som_x, som_y, win_map, name):\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=som_x, cols=som_y,\n",
    "        shared_xaxes=True,\n",
    "        shared_yaxes=True,\n",
    "        vertical_spacing=0.2,\n",
    "    )\n",
    "    \n",
    "    # Time\n",
    "    time = np.linspace(0,150, 900, endpoint=True)\n",
    "    \n",
    "    # Create the subplots\n",
    "    for x in range(som_x):\n",
    "        for y in range(som_y):\n",
    "            cluster = (x,y)\n",
    "            cluster_number = x*som_y+y\n",
    "            if cluster in win_map.keys():\n",
    "                for series in win_map[cluster]:    \n",
    "                    \n",
    "                    # Plot the traces\n",
    "                    fig.add_trace(go.Scatter(x=time, y=series, mode='lines',\n",
    "                                             name=f\"Cluster {cluster_number}\",\n",
    "                                             line_color='rgba(130,179,196,0.12)',\n",
    "                                             showlegend=False),\n",
    "                                  row=x+1, col=y+1)\n",
    "                \n",
    "                # Calculate the barycenter average\n",
    "                cluster_dtw = np.transpose(dtw_barycenter_averaging(np.vstack(win_map[cluster])))\n",
    "                \n",
    "                # Plot the barycenter average\n",
    "                fig.add_trace(go.Scatter(x=time, y= cluster_dtw[0], mode='lines',\n",
    "                                         name=f\"Cluster dtw {cluster_dtw}\",\n",
    "                                         line_color='rgb(57,103,119)',\n",
    "                                         showlegend=False),\n",
    "                              row=x+1, col=y+1)\n",
    "                    \n",
    "            # Update the figure\n",
    "            fig.update_yaxes(range=[-0.1,1.1], row=x+1, col=y+1)\n",
    "            fig.update_layout(font_family='Arial')\n",
    "\n",
    "    # Save the figure\n",
    "    pio.write_image(fig, name+'barryaverage.png', width=1*600, height=600, scale=15)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672643fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up name, sigma, learning_rate\n",
    "sigma = 0.5\n",
    "learning_rate= 0.1\n",
    "\n",
    "# Set the number of clusters\n",
    "som_x = 2\n",
    "som_y = 2\n",
    "cluster_count= som_x*som_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e05c32",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reset sns \n",
    "sns.reset_orig()\n",
    "\n",
    "# Calculate the SOM\n",
    "som = MiniSom(som_x, som_y,len(mySeriesDrop_savgol[0]), sigma=sigma, learning_rate = learning_rate)\n",
    "\n",
    "som.random_weights_init(mySeriesDrop_savgol)\n",
    "som.train(mySeriesDrop_savgol, 50000)\n",
    "\n",
    "# Plot savgol\n",
    "win_map = som.win_map(mySeriesDrop_savgol)\n",
    "\n",
    "# Returns the mapping of the winner nodes and inputs\n",
    "plot_som_series_averaged_center(som_x, som_y, win_map, filedirname)\n",
    "# plot_som_series_dba_center(som_x, som_y, win_map, filedirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e65981d",
   "metadata": {},
   "source": [
    "Now, after doing the clustering, let's store the data in pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335a88b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find out which data row belongs to which cluster\n",
    "som_shape = (som_x,som_y)\n",
    "winner_coordinates = np.array([som.winner(x) for x in mySeriesDrop_savgol]).T\n",
    "\n",
    "# With np.ravel_multi_index we convert the 2-dimensional\n",
    "# coordinates to a 1-dimensional index\n",
    "cluster_index = np.ravel_multi_index(winner_coordinates, som_shape)\n",
    "\n",
    "# Identify the number of clusters\n",
    "cluster_c = [len(cluster_index[cluster_index==i]) for i in np.unique(cluster_index)]\n",
    "cluster_n = [\"cluster_\"+str(i) for i in np.unique(cluster_index)]\n",
    "\n",
    "fancy_names_for_labels = [f\"{label}\" for label in cluster_index]\n",
    "result = pd.DataFrame(zip(mySeries['Filename'],mySeries['Pixel'],\n",
    "                          mySeries['SampleNumber'],fancy_names_for_labels),\n",
    "                      columns=[\"Series\",'Pixel',\"SampleNumber\",\"Cluster\"]).sort_values(by=\"Cluster\")#.set_index(\"Series\")\n",
    "\n",
    "result['PCE_before_x'] = PCE_df['PCE_before_x']\n",
    "result['PCE_before_ceil_x'] = PCE_df['PCE_before_ceil_x']\n",
    "result['PCE_before_median_x'] = PCE_df['PCE_before_median_x']\n",
    "result['PCE_before_mean_x'] = PCE_df['PCE_before_mean_x']\n",
    "result['PCE_delta'] = PCE_df['PCE_delta']\n",
    "\n",
    "# Save result on the .csv file\n",
    "(result.sort_index()).to_csv(filedirname+'clusters.csv')\n",
    "\n",
    "result.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f40d316",
   "metadata": {},
   "source": [
    "### 3.2. General SOM cluster plots\n",
    "\n",
    "Now, let's plot some of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a79c21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Plot cluster distribution vertical\n",
    "fig = px.bar(x=cluster_n, y=cluster_c,labels=dict(x='Clusters',y='Count'))\n",
    "\n",
    "fig.update_layout(font_family='Arial')\n",
    "fig.update_traces(marker_color='rgba(57,103,119,0.7)')\n",
    "\n",
    "# Save the figure\n",
    "pio.write_image(fig, filedirname+'distribution_v.png',\n",
    "                width=1*400, height=400, scale=16)\n",
    "fig.show()\n",
    "\n",
    "# Plot horizontal distribution \n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    y=cluster_n,\n",
    "    x=cluster_c,\n",
    "    orientation='h',\n",
    "    marker=dict(\n",
    "        color= 'rgba(57,103,119,0.7)',\n",
    "    )\n",
    "))\n",
    "\n",
    "fig.update_layout(font_family='Arial',\n",
    "                  xaxis=dict(title='Count'),\n",
    "                  yaxis=dict(title='Cluster'))\n",
    "\n",
    "pio.write_image(fig, filedirname+'distribution_h.png', width=1*400, height=1*400, scale=16)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c32a487",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Colormap PCE\n",
    "colormap_PCE = {1.0: \"#034e7b\",\n",
    "                2.0: \"#0570b0\",\n",
    "                3.0: \"#3690c0\",\n",
    "                4.0: \"#74a9cf\",\n",
    "                5.0: \"#a6bddb\"}\n",
    "\n",
    "# Colormap cluster\n",
    "colormap_cluster = {\"0\": px.colors.qualitative.Antique[4],\n",
    "                    \"1\": px.colors.qualitative.Antique[9],\n",
    "                    \"2\": px.colors.qualitative.Antique[6],\n",
    "                    \"3\": px.colors.qualitative.Antique[8]}\n",
    "\n",
    "# Plot histogram for PCE group distribution\n",
    "fig = px.histogram(result.sort_values(by=['Cluster','PCE_before_x']),\n",
    "                   x=\"Cluster\", color='PCE_before_x',\n",
    "                   color_discrete_map=colormap_PCE, opacity=0.75,\n",
    "#                    histnorm='percent',\n",
    "                  )\n",
    "# Update layout\n",
    "fig.update_layout(xaxis_title=\"Cluster\",\n",
    "                  legend_title = 'Max. PCE Group',\n",
    "                  yaxis_title=\"Count\",\n",
    "                  font_family='Arial',barmode='group')#,barnorm='fraction')\n",
    "\n",
    "# Save figure\n",
    "pio.write_image(fig, filedirname+'group_2.png', width=1*400, height=1*400, scale=16)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Plot histogram for cluster distribution\n",
    "fig = px.histogram(result.sort_values(by=['Cluster','PCE_before_x']),\n",
    "                   x=\"PCE_before_x\", color='Cluster',\n",
    "                   color_discrete_map=colormap_cluster, opacity=0.7,\n",
    "#                    histnorm='percent',\n",
    "                  )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(xaxis_title=\"Max. PCE Group (%)\",# xaxis=dict(range=[7,25]),\n",
    "                  yaxis_title=\"Count\", font_family='Arial',barmode='group')#,barnorm='fraction')\n",
    "\n",
    "# Save figure\n",
    "pio.write_image(fig, filedirname+'group_3.png', width=1*400, height=1*400, scale=16)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076782e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Colormap PCE\n",
    "colormap_PCE = {1.0: \"#034e7b\",\n",
    "                2.0: \"#0570b0\",\n",
    "                3.0: \"#3690c0\",\n",
    "                4.0: \"#74a9cf\",\n",
    "                5.0: \"#a6bddb\"}\n",
    "\n",
    "# Colormap cluster\n",
    "colormap_cluster = {\"0\": px.colors.qualitative.Antique[4],\n",
    "                    \"1\": px.colors.qualitative.Antique[9],\n",
    "                    \"2\": px.colors.qualitative.Antique[6],\n",
    "                    \"3\": px.colors.qualitative.Antique[8]}\n",
    "\n",
    "# Plot stacked bar based on PCE group\n",
    "fig = px.histogram(result.sort_values(by=['Cluster','PCE_before_x']),\n",
    "                   x=\"Cluster\", color='PCE_before_x',\n",
    "                   color_discrete_map=colormap_PCE, opacity=0.75,\n",
    "                   histnorm='percent',\n",
    "                  )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(xaxis_title=\"Cluster\",\n",
    "                  legend_title = 'Max. PCE Group',\n",
    "                  yaxis_title=\"Count\",\n",
    "                  font_family='Arial')#,barmode='group')#,barnorm='fraction')\n",
    "\n",
    "# Save figure\n",
    "pio.write_image(fig, filedirname+'percent_2.png', width=1*400, height=1*400, scale=16)\n",
    "fig.show()\n",
    "\n",
    "# Plot stacked bar based on cluster\n",
    "fig = px.histogram(result.sort_values(by=['Cluster','PCE_before_x']),\n",
    "                   x=\"PCE_before_x\", color='Cluster',\n",
    "                   color_discrete_map=colormap_cluster, opacity=0.75,\n",
    "                   histnorm='percent',\n",
    "                  )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(xaxis_title=\"Max. PCE Group\",#xaxis=dict(range=[7,26]),\n",
    "                  yaxis_title=\"Count\", font_family='Arial',bargap=0.1)#,barmode='group')#,barnorm='fraction')\n",
    "\n",
    "# Save figure\n",
    "pio.write_image(fig, filedirname+'percent_3.png', width=1*400, height=1*400, scale=16)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7e1ff7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Colormap PCE\n",
    "colormap_PCE = {1.0: \"#034e7b\",\n",
    "                2.0: \"#0570b0\",\n",
    "                3.0: \"#3690c0\",\n",
    "                4.0: \"#74a9cf\",\n",
    "                5.0: \"#a6bddb\"}\n",
    "\n",
    "# Colormap cluster\n",
    "colormap_cluster = {\"0\": px.colors.qualitative.Antique[4],\n",
    "                    \"1\": px.colors.qualitative.Antique[9],\n",
    "                    \"2\": px.colors.qualitative.Antique[6],\n",
    "                    \"3\": px.colors.qualitative.Antique[8]}\n",
    "\n",
    "# Plot normalized bar based on PCE group\n",
    "fig = px.histogram(result.sort_values(by=['Cluster','PCE_before_x']),\n",
    "                   x=\"Cluster\", color='PCE_before_x',\n",
    "                   color_discrete_map=colormap_PCE, opacity=0.75,\n",
    "#                    histnorm='percent',\n",
    "                  )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(xaxis_title=\"Cluster\",\n",
    "                  legend_title = 'Max. PCE Group',\n",
    "                  yaxis_title=\"Count\",\n",
    "                  font_family='Arial',barnorm='fraction')\n",
    "\n",
    "# Save figure\n",
    "pio.write_image(fig, filedirname+'fraction_2.png', width=1*400, height=1*400, scale=16)\n",
    "fig.show()\n",
    "\n",
    "# Plot normalized bar based on cluster\n",
    "fig = px.histogram(result.sort_values(by=['Cluster','PCE_before_x']),\n",
    "                   x=\"PCE_before_x\", color='Cluster',\n",
    "                   color_discrete_map=colormap_cluster, opacity=0.75,\n",
    "#                    histnorm='percent',\n",
    "                  )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(xaxis_title=\"Max. PCE Group\",#xaxis=dict(range=[9,26]),\n",
    "                  yaxis_title=\"Count\", font_family='Arial',barnorm='fraction',\n",
    "                  bargap=0.1)\n",
    "\n",
    "# Save figure\n",
    "pio.write_image(fig, filedirname+'fraction_3.png', width=1*400, height=1*400, scale=16)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5702bd0c",
   "metadata": {},
   "source": [
    "### 3.3. Split based on n-i-p or p-i-n\n",
    "\n",
    "Let's split based on the architecture (n-i-p: the 'normal' architecture, p-i-n: the 'inverted' architecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa2a01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert FrontContact to the result dataframe\n",
    "result_arch = pd.concat([result.sort_index(),\n",
    "                         mySeries['FrontContact'].reset_index(drop=True)],\n",
    "                        axis=1)\n",
    "\n",
    "# Save it as a new list\n",
    "(result_arch.sort_index()).to_csv(filedirname+'architecture.csv')\n",
    "result_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d66adfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to plot the histogram based on cluster\n",
    "\n",
    "def plot_histogram_cluster_plotly(som_x, som_y, result_arch,name):\n",
    "    \n",
    "    # Create subplots and define their properties\n",
    "    fig = make_subplots(\n",
    "        rows=som_x, cols=som_y,\n",
    "        shared_xaxes=True,\n",
    "        shared_yaxes=True,\n",
    "        vertical_spacing=0.2, #0.03\n",
    "        specs=[[{\"type\": \"scatter\"},\n",
    "                {\"type\": \"scatter\"}],\n",
    "               [{\"type\": \"scatter\"},\n",
    "                {\"type\": \"scatter\"}]],\n",
    "    )\n",
    "    \n",
    "    # Going through SOM results and plot them\n",
    "    for x in range(som_x):\n",
    "        for y in range(som_y):\n",
    "            cluster = (x,y)\n",
    "            cluster_number = x*som_y+y\n",
    "            \n",
    "            # Select specific cluster\n",
    "            selected = result_arch.loc[result_arch['Cluster'] == str(cluster_number)]\n",
    "            \n",
    "            # Select the ITO-based devices\n",
    "            selected_ITO = selected.loc[selected['FrontContact'] == 'ITO']\n",
    "            \n",
    "            # Select the FTO-based devices\n",
    "            selected_FTO = selected.loc[selected['FrontContact'] == 'FTO']\n",
    "            \n",
    "            # Plot the histograms\n",
    "            # Plot the ITO\n",
    "            fig.add_trace(go.Histogram(x = selected_ITO['PCE_before_x'],\n",
    "                                       marker_color='rgb(57,103,119)',\n",
    "                                       opacity=0.75),\n",
    "                          row=x+1,col=y+1)\n",
    "            # Plot the FTO\n",
    "            fig.add_trace(go.Histogram(x = selected_FTO['PCE_before_x'],\n",
    "                                       marker_color='rgb(130,179,196,0.12)',\n",
    "                                       opacity=0.75),\n",
    "                          row=x+1,col=y+1)\n",
    "    \n",
    "    # Update figure properties\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    fig.update_yaxes(range=[0,380])\n",
    "    fig.update_layout(boxgap=0.05,\n",
    "                      font_family='Arial',\n",
    "                      showlegend=False)\n",
    "    \n",
    "    pio.write_image(fig, name, width=1*400, height=400, scale=16)\n",
    "\n",
    "    fig.show()\n",
    "    \n",
    "    return selected\n",
    "\n",
    "def plot_histogram_cluster_plotly_percent_subplot(som_x, som_y, result_arch,name):\n",
    "    \n",
    "    # Create subplots and define their properties\n",
    "    fig = make_subplots(\n",
    "        rows=som_x, cols=som_y,\n",
    "        shared_xaxes=True,\n",
    "        shared_yaxes=True,\n",
    "        vertical_spacing=0.06,#0.2, #0.03\n",
    "        specs=[[{\"type\": \"scatter\"},\n",
    "                {\"type\": \"scatter\"}],\n",
    "               [{\"type\": \"scatter\"},\n",
    "                {\"type\": \"scatter\"}]],\n",
    "    )\n",
    "    \n",
    "    # Going through SOM results and plot them\n",
    "    for x in range(som_x):\n",
    "        for y in range(som_y):\n",
    "            cluster = (x,y)\n",
    "            cluster_number = x*som_y+y\n",
    "            \n",
    "            # Select specific cluster\n",
    "            selected = result_arch.loc[result_arch['Cluster'] == str(cluster_number)]\n",
    "            \n",
    "            # Plot the histograms\n",
    "            fig.add_trace(go.Histogram(x = selected['FrontContact'],\n",
    "                                       opacity=0.75),\n",
    "                          row=x+1,col=y+1)\n",
    "    \n",
    "    # Update figure properties\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    fig.update_layout(boxgap=0.2,\n",
    "                      font_family='Arial',\n",
    "                      showlegend=False)\n",
    "    \n",
    "    pio.write_image(fig, name, width=1*400, height=400, scale=16)\n",
    "\n",
    "    fig.show()\n",
    "    \n",
    "    return selected\n",
    "\n",
    "def plot_histogram_cluster_plotly_percent_all(som_x, som_y, result_arch,name):\n",
    "    \n",
    "    # Define colormap for each of the PCE\n",
    "    colormap_PCE = {1.0: \"#034e7b\",\n",
    "                    2.0: \"#0570b0\",\n",
    "                    3.0: \"#3690c0\",\n",
    "                    4.0: \"#74a9cf\",\n",
    "                    5.0: \"#a6bddb\"}\n",
    "    \n",
    "    # Create subplots and define their properties\n",
    "    fig = make_subplots(\n",
    "        rows=som_x, cols=som_y,\n",
    "        shared_xaxes=True,\n",
    "        shared_yaxes=True,\n",
    "        vertical_spacing=0.1,#0.2, #0.03\n",
    "        horizontal_spacing = 0.1,\n",
    "        specs=[[{\"type\": \"scatter\"},\n",
    "                {\"type\": \"scatter\"}],\n",
    "               [{\"type\": \"scatter\"},\n",
    "                {\"type\": \"scatter\"}]],\n",
    "    )\n",
    "    \n",
    "    # Going through SOM results and plot them\n",
    "    for x in range(som_x):\n",
    "        for y in range(som_y):\n",
    "            cluster = (x,y)\n",
    "            cluster_number = x*som_y+y\n",
    "            \n",
    "            # Select specific cluster\n",
    "            selected = result_arch.loc[result_arch['Cluster'] == str(cluster_number)]\n",
    "            \n",
    "            # Select specific max. PCE group\n",
    "            selected_1 = selected.loc[selected['PCE_before_x'] == 1]\n",
    "            selected_2 = selected.loc[selected['PCE_before_x'] == 2]\n",
    "            selected_3 = selected.loc[selected['PCE_before_x'] == 3]\n",
    "            selected_4 = selected.loc[selected['PCE_before_x'] == 4]\n",
    "            selected_5 = selected.loc[selected['PCE_before_x'] == 5]\n",
    "            \n",
    "            # Plot the bar plot\n",
    "            fig.add_trace(go.Bar(x = selected_1.sort_values(by=['Cluster','PCE_before_x'])['FrontContact'],\n",
    "                                 y = selected_1.sort_values(by=['Cluster','PCE_before_x'])['PCE_before_ceil_x'],\n",
    "                                 offsetgroup = 0),\n",
    "                          row=x+1, col=y+1)\n",
    "            \n",
    "            fig.add_trace(go.Bar(x = selected_2.sort_values(by=['Cluster','PCE_before_x'])['FrontContact'],\n",
    "                                 y = selected_2.sort_values(by=['Cluster','PCE_before_x'])['PCE_before_ceil_x'],\n",
    "                                 offsetgroup = 0, base = selected_1.sort_values(by=['Cluster','PCE_before_x'])['PCE_before_x']),\n",
    "                          row=x+1, col=y+1)\n",
    "            \n",
    "            fig.add_trace(go.Bar(x = selected_3.sort_values(by=['Cluster','PCE_before_x'])['FrontContact'],\n",
    "                                 y = selected_3.sort_values(by=['Cluster','PCE_before_x'])['PCE_before_ceil_x'],\n",
    "                                 offsetgroup = 0, base = selected_2.sort_values(by=['Cluster','PCE_before_x'])['PCE_before_x']),\n",
    "                          row=x+1, col=y+1)\n",
    "            \n",
    "            fig.add_trace(go.Bar(x = selected_4.sort_values(by=['Cluster','PCE_before_x'])['FrontContact'],\n",
    "                                 y = selected_4.sort_values(by=['Cluster','PCE_before_x'])['PCE_before_ceil_x'],\n",
    "                                 offsetgroup = 0, base = selected_3.sort_values(by=['Cluster','PCE_before_x'])['PCE_before_x']),\n",
    "                          row=x+1, col=y+1)\n",
    "            \n",
    "            fig.add_trace(go.Bar(x = selected_5.sort_values(by=['Cluster','PCE_before_x'])['FrontContact'],\n",
    "                                 y = selected_5.sort_values(by=['Cluster','PCE_before_x'])['PCE_before_ceil_x'],\n",
    "                                 offsetgroup = 0, base = selected_4.sort_values(by=['Cluster','PCE_before_x'])['PCE_before_x']),\n",
    "                          row=x+1, col=y+1)\n",
    "    \n",
    "    # Update the figure properties\n",
    "        fig.update_layout(font_family='Arial',\n",
    "                      showlegend=False)\n",
    "    \n",
    "    pio.write_image(fig, name, width=1*400, height=400, scale=16)\n",
    "\n",
    "    fig.show()\n",
    "    \n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4361d646",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the 2 architectures based on cluster\n",
    "selected = plot_histogram_cluster_plotly(som_x, som_y, result_arch,filedirname+'histogram_contact.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b86ef3a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the clusters based on the architectures\n",
    "selected = plot_histogram_cluster_plotly_percent_all(som_x, som_y, result_arch,\n",
    "                                                     filedirname+'histogram_contact_percentage.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c637a786",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Separated by cluster, for each max. PCE group\n",
    "\n",
    "colormap_PCE = ['#034e7b', '#0570b0', '#3690c0', '#74a9cf', '#a6bddb']\n",
    "opacity = 0.75\n",
    "\n",
    "x = ['n-i-p', 'p-i-n']\n",
    "\n",
    "n_cluster = result_arch['Cluster'].nunique()\n",
    "groupCount = result_arch.groupby(['Cluster','PCE_before_x','FrontContact'])['Cluster'].count()\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=som_x, cols=som_y,\n",
    "    shared_xaxes=True,\n",
    "    shared_yaxes=True,\n",
    "    vertical_spacing=0.1,#0.2, #0.03\n",
    "    horizontal_spacing = 0.1,\n",
    "    specs=[[{\"type\": \"scatter\"},\n",
    "            {\"type\": \"scatter\"}],\n",
    "           [{\"type\": \"scatter\"},\n",
    "            {\"type\": \"scatter\"}]],\n",
    "    )\n",
    "\n",
    "# See if certain cluster is non-existent\n",
    "if {'1',4.0,'FTO'}.issubset(groupCount) == False:\n",
    "    val_1 = 0\n",
    "else:\n",
    "    val_1 = groupCount['1'][4.0]['FTO']\n",
    "    \n",
    "if {'2',3.0,'FTO'}.issubset(groupCount) == False:\n",
    "    val_2 = 0\n",
    "else:\n",
    "    val_2 = groupCount['2'][3.0]['FTO']\n",
    "    \n",
    "if {'2',4.0,'FTO'}.issubset(groupCount) == False:\n",
    "    val_3 = 0\n",
    "else:\n",
    "    val_3 = groupCount['2'][4.0]['FTO']\n",
    "    \n",
    "if {'3',4.0,'FTO'}.issubset(groupCount) == False:\n",
    "    val_4 = 0\n",
    "else:\n",
    "    val_4 = groupCount['3'][4.0]['FTO']\n",
    "\n",
    "if {'1',3.0,'FTO'}.issubset(groupCount) == False:\n",
    "    val_5 = 0\n",
    "else:\n",
    "    val_5 = groupCount['1'][3.0]['FTO']\n",
    "\n",
    "if {'1',5.0,'FTO'}.issubset(groupCount) == False:\n",
    "    val_6 = 0\n",
    "else:\n",
    "    val_6 = groupCount['1'][5.0]['FTO']\n",
    "\n",
    "if {'2',5.0,'FTO'}.issubset(groupCount) == False:\n",
    "    val_7 = 0\n",
    "else:\n",
    "    val_7 = groupCount['2'][5.0]['FTO']\n",
    "\n",
    "if {'3',5.0,'FTO'}.issubset(groupCount) == False:\n",
    "    val_8 = 0\n",
    "else:\n",
    "    val_8 = groupCount['3'][5.0]['FTO']\n",
    "\n",
    "if {'3',5.0,'ITO'}.issubset(groupCount) == False:\n",
    "    val_9 = 0\n",
    "else:\n",
    "    val_9 = groupCount['3'][5.0]['ITO']\n",
    "\n",
    "if {'3',3.0,'FTO'}.issubset(groupCount) == False:\n",
    "    val_10 = 0\n",
    "else:\n",
    "    val_10 = groupCount['3'][3.0]['FTO']\n",
    "\n",
    "if {'0',3.0,'FTO'}.issubset(groupCount) == False:\n",
    "    val_11 = 0\n",
    "else:\n",
    "    val_11 = groupCount['0'][3.0]['FTO']\n",
    "    \n",
    "if {'0',4.0,'FTO'}.issubset(groupCount) == False:\n",
    "    val_12 = 0\n",
    "else:\n",
    "    val_12 = groupCount['0'][4.0]['FTO']\n",
    "    \n",
    "if {'0',5.0,'FTO'}.issubset(groupCount) == False:\n",
    "    val_13 = 0\n",
    "else:\n",
    "    val_13 = groupCount['0'][5.0]['FTO']\n",
    "\n",
    "if {'2',5.0,'ITO'}.issubset(groupCount) == False:\n",
    "    val_14 = 0\n",
    "else:\n",
    "    val_14 = groupCount['2'][5.0]['ITO']\n",
    "\n",
    "# Cluster 1\n",
    "fig.add_trace(go.Bar(x=x, y=[groupCount['0'][1.0]['FTO'],\n",
    "                             groupCount['0'][1.0]['ITO']],\n",
    "                     name='< 10%', marker_color=colormap_PCE[0],\n",
    "                     opacity=opacity),\n",
    "              row=1, col=1)\n",
    "fig.add_trace(go.Bar(x=x, y=[groupCount['0'][2.0]['FTO'],\n",
    "                             groupCount['0'][2.0]['ITO']], \n",
    "                     name='10-14%', marker_color=colormap_PCE[1],\n",
    "                     opacity=opacity),\n",
    "              row=1, col=1)\n",
    "fig.add_trace(go.Bar(x=x, y=[val_11,\n",
    "                             groupCount['0'][3.0]['ITO']], \n",
    "                     name='14-16.6%', marker_color=colormap_PCE[2],\n",
    "                     opacity=opacity),\n",
    "              row=1, col=1)\n",
    "fig.add_trace(go.Bar(x=x, y=[val_12,\n",
    "                             groupCount['0'][4.0]['ITO']], \n",
    "                     name='16.6-19.2%', marker_color=colormap_PCE[3],\n",
    "                     opacity=opacity),\n",
    "              row=1, col=1)\n",
    "fig.add_trace(go.Bar(x=x, y=[val_13,\n",
    "                             groupCount['0'][5.0]['ITO']], \n",
    "                     name='> 19.2%', marker_color=colormap_PCE[4],\n",
    "                     opacity=opacity),\n",
    "              row=1, col=1)\n",
    "\n",
    "\n",
    "# Cluster 2    \n",
    "fig.add_trace(go.Bar(x=x, y=[groupCount['1'][1.0]['FTO'],\n",
    "                             groupCount['1'][1.0]['ITO']],\n",
    "                     name='< 10%', marker_color=colormap_PCE[0],\n",
    "                     opacity=opacity),\n",
    "              row=1, col=2)\n",
    "fig.add_trace(go.Bar(x=x, y=[groupCount['1'][2.0]['FTO'],\n",
    "                             groupCount['1'][2.0]['ITO']],\n",
    "                     name='10-14%', marker_color=colormap_PCE[1],\n",
    "                     opacity=opacity),\n",
    "              row=1, col=2)\n",
    "fig.add_trace(go.Bar(x=x, y=[val_5,\n",
    "                             groupCount['1'][3.0]['ITO']],\n",
    "                     name='14-16.6%', marker_color=colormap_PCE[2],\n",
    "                     opacity=opacity),\n",
    "              row=1, col=2)\n",
    "fig.add_trace(go.Bar(x=x, y=[val_1,\n",
    "                             groupCount['1'][4.0]['ITO']], \n",
    "                     name='16.6-19.2%', marker_color=colormap_PCE[3],\n",
    "                     opacity=opacity),\n",
    "              row=1, col=2)\n",
    "fig.add_trace(go.Bar(x=x, y=[val_6,\n",
    "                             groupCount['1'][5.0]['ITO']], \n",
    "                     name='> 19.2%', marker_color=colormap_PCE[4],\n",
    "                     opacity=opacity),\n",
    "              row=1, col=2)\n",
    "\n",
    "# Cluster 3    \n",
    "fig.add_trace(go.Bar(x=x, y=[groupCount['2'][1.0]['FTO'],\n",
    "                             groupCount['2'][1.0]['ITO']],\n",
    "                     name='< 10%', marker_color=colormap_PCE[0],\n",
    "                     opacity=opacity),\n",
    "              row=2, col=1)\n",
    "fig.add_trace(go.Bar(x=x, y=[groupCount['2'][2.0]['FTO'],\n",
    "                             groupCount['2'][2.0]['ITO']],\n",
    "                     name='10-14%', marker_color=colormap_PCE[1],\n",
    "                     opacity=opacity),\n",
    "              row=2, col=1)\n",
    "fig.add_trace(go.Bar(x=x, y=[val_2,\n",
    "                             groupCount['2'][3.0]['ITO']],\n",
    "                     name='14-16.6%', marker_color=colormap_PCE[2],\n",
    "                     opacity=opacity),\n",
    "              row=2, col=1)\n",
    "fig.add_trace(go.Bar(x=x, y=[val_3,\n",
    "                             groupCount['2'][4.0]['ITO']],\n",
    "                     name='16.6-19.2%', marker_color=colormap_PCE[3],\n",
    "                     opacity=opacity),\n",
    "              row=2, col=1)\n",
    "fig.add_trace(go.Bar(x=x, y=[val_7,\n",
    "                             val_14],\n",
    "                     name='> 19.2%', marker_color=colormap_PCE[4],\n",
    "                     opacity=opacity),\n",
    "              row=2, col=1)\n",
    "\n",
    "# Cluster 4 \n",
    "fig.add_trace(go.Bar(x=x, y=[groupCount['3'][1.0]['FTO'],\n",
    "                             groupCount['3'][1.0]['ITO']],\n",
    "                     name='< 10%', marker_color=colormap_PCE[0],\n",
    "                     opacity=opacity),\n",
    "              row=2, col=2)\n",
    "fig.add_trace(go.Bar(x=x, y=[groupCount['3'][2.0]['FTO'],\n",
    "                             groupCount['3'][2.0]['ITO']], \n",
    "                     name='10-14%', marker_color=colormap_PCE[1],\n",
    "                     opacity=opacity),\n",
    "              row=2, col=2)\n",
    "fig.add_trace(go.Bar(x=x, y=[val_10,\n",
    "                             groupCount['3'][3.0]['ITO']],\n",
    "                     name='14-16.6%', marker_color=colormap_PCE[2],\n",
    "                     opacity=opacity),\n",
    "              row=2, col=2)\n",
    "fig.add_trace(go.Bar(x=x, y=[val_4,\n",
    "                             groupCount['3'][4.0]['ITO']], \n",
    "                     name='16.6-19.2%', marker_color=colormap_PCE[3],\n",
    "                     opacity=opacity),\n",
    "              row=2, col=2)\n",
    "fig.add_trace(go.Bar(x=x, y=[val_8,\n",
    "                             val_9], \n",
    "                     name='> 19.2%', marker_color=colormap_PCE[4],\n",
    "                     opacity=opacity),\n",
    "              row=2, col=2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(barmode='stack', font_family='Arial', showlegend=False)\n",
    "\n",
    "# Save figure\n",
    "pio.write_image(fig, filedirname+'cluster_nip.png', width=1*600, height=600, scale=12)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68553418",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Cluster-based, percent\n",
    "\n",
    "colormap_PCE = ['#034e7b', '#0570b0', '#3690c0', '#74a9cf', '#a6bddb']\n",
    "colormap_cluster = ['#045a8d', '#2b8cbe', '#74a9cf', '#bdc9e1']\n",
    "opacity = 0.75\n",
    "width = 0.5\n",
    "\n",
    "x = ['Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4']\n",
    "\n",
    "n_cluster = result_arch['Cluster'].nunique()\n",
    "groupCount = result_arch.groupby(['Cluster','PCE_before_x','FrontContact'])['Cluster'].count()\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    shared_xaxes=False,\n",
    "    shared_yaxes=True,\n",
    "    horizontal_spacing = 0.005,\n",
    "    )\n",
    "\n",
    "# n-i-p\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['0'][1.0]['FTO'],\n",
    "                            groupCount['1'][1.0]['FTO'],\n",
    "                            groupCount['2'][1.0]['FTO'],\n",
    "                            groupCount['3'][1.0]['FTO']],\n",
    "                     name='< 10%',marker_color=colormap_PCE[0],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=1)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['0'][2.0]['FTO'],\n",
    "                            groupCount['1'][2.0]['FTO'],\n",
    "                            groupCount['2'][2.0]['FTO'],\n",
    "                            groupCount['3'][2.0]['FTO']],\n",
    "                     name='10-14%',marker_color=colormap_PCE[1],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=1)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[val_11,\n",
    "                            val_5,\n",
    "                            val_2,\n",
    "                            val_10],\n",
    "                     name='14-16.6%',marker_color=colormap_PCE[2],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=1)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[val_12,\n",
    "                            val_1,\n",
    "                            val_3,\n",
    "                            val_4],\n",
    "                     name='16.6-19.2%',marker_color=colormap_PCE[3],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=1)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[val_13,\n",
    "                            val_6,\n",
    "                            val_7,\n",
    "                            val_8],\n",
    "                     name='> 19.2%',marker_color=colormap_PCE[4],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=1)\n",
    "\n",
    "# p-i-n\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['0'][1.0]['ITO'],\n",
    "                            groupCount['1'][1.0]['ITO'],\n",
    "                            groupCount['2'][1.0]['ITO'],\n",
    "                            groupCount['3'][1.0]['ITO']],\n",
    "                     name='< 10%',marker_color=colormap_PCE[0],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=2)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['0'][2.0]['ITO'],\n",
    "                            groupCount['1'][2.0]['ITO'],\n",
    "                            groupCount['2'][2.0]['ITO'],\n",
    "                            groupCount['3'][2.0]['ITO']],\n",
    "                     name='10-14%',marker_color=colormap_PCE[1],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=2)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['0'][3.0]['ITO'],\n",
    "                            groupCount['1'][3.0]['ITO'],\n",
    "                            groupCount['2'][3.0]['ITO'],\n",
    "                            groupCount['3'][3.0]['ITO']],\n",
    "                     name='14-16.6%',marker_color=colormap_PCE[2],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=2)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['0'][4.0]['ITO'],\n",
    "                            groupCount['1'][4.0]['ITO'],\n",
    "                            groupCount['2'][4.0]['ITO'],\n",
    "                            groupCount['3'][4.0]['ITO']],\n",
    "                     name='16.6-19.2%',marker_color=colormap_PCE[3],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=2)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['0'][5.0]['ITO'],\n",
    "                            groupCount['1'][5.0]['ITO'],\n",
    "                            val_14,\n",
    "                            val_9],\n",
    "                     name='> 19.2%',marker_color=colormap_PCE[4],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=2)\n",
    "\n",
    "fig.update_xaxes(showticklabels=False,title_text=\"n-i-p\", row=1, col=1, range=[1.05,0])\n",
    "fig.update_xaxes(showticklabels=False,title_text=\"p-i-n\", row=1, col=2, range=[0,1.05])\n",
    "\n",
    "fig.update_layout(barmode='relative', barnorm='fraction',font_family='Arial', showlegend=False, #barmode='stack'\n",
    "                  xaxis1={'side': 'top'},\n",
    "                  xaxis2={'side': 'top'},)\n",
    "\n",
    "pio.write_image(fig, filedirname+'butterfly_arch_cluster_based_percent.png', width=1*600, height=0.75*600, scale=12)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767ce085",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cluster based, count\n",
    "\n",
    "colormap_PCE = ['#034e7b', '#0570b0', '#3690c0', '#74a9cf', '#a6bddb']\n",
    "colormap_cluster = ['#045a8d', '#2b8cbe', '#74a9cf', '#bdc9e1']\n",
    "opacity = 0.75\n",
    "width = 0.5\n",
    "\n",
    "x = ['Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4']\n",
    "\n",
    "n_cluster = result_arch['Cluster'].nunique()\n",
    "groupCount = result_arch.groupby(['Cluster','PCE_before_x','FrontContact'])['Cluster'].count()\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    shared_xaxes=False,\n",
    "    shared_yaxes=True,\n",
    "    horizontal_spacing = 0.005,\n",
    "    )\n",
    "\n",
    "# n-i-p\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['0'][1.0]['FTO'],\n",
    "                            groupCount['1'][1.0]['FTO'],\n",
    "                            groupCount['2'][1.0]['FTO'],\n",
    "                            groupCount['3'][1.0]['FTO']],\n",
    "                     name='< 10%',marker_color=colormap_PCE[0],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=1)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['0'][2.0]['FTO'],\n",
    "                            groupCount['1'][2.0]['FTO'],\n",
    "                            groupCount['2'][2.0]['FTO'],\n",
    "                            groupCount['3'][2.0]['FTO']],\n",
    "                     name='10-14%',marker_color=colormap_PCE[1],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=1)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[val_11,\n",
    "                            val_5,\n",
    "                            val_2,\n",
    "                            val_10],\n",
    "                     name='14-16.6%',marker_color=colormap_PCE[2],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=1)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[val_12,\n",
    "                            val_1,\n",
    "                            val_3,\n",
    "                            val_4],\n",
    "                     name='16.6-19.2%',marker_color=colormap_PCE[3],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=1)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[val_13,\n",
    "                            val_6,\n",
    "                            val_7,\n",
    "                            val_8],\n",
    "                     name='> 19.2%',marker_color=colormap_PCE[4],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=1)\n",
    "\n",
    "# p-i-n\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['0'][1.0]['ITO'],\n",
    "                            groupCount['1'][1.0]['ITO'],\n",
    "                            groupCount['2'][1.0]['ITO'],\n",
    "                            groupCount['3'][1.0]['ITO']],\n",
    "                     name='< 10%',marker_color=colormap_PCE[0],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=2)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['0'][2.0]['ITO'],\n",
    "                            groupCount['1'][2.0]['ITO'],\n",
    "                            groupCount['2'][2.0]['ITO'],\n",
    "                            groupCount['3'][2.0]['ITO']],\n",
    "                     name='10-14%',marker_color=colormap_PCE[1],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=2)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['0'][3.0]['ITO'],\n",
    "                            groupCount['1'][3.0]['ITO'],\n",
    "                            groupCount['2'][3.0]['ITO'],\n",
    "                            groupCount['3'][3.0]['ITO']],\n",
    "                     name='14-16.6%',marker_color=colormap_PCE[2],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=2)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['0'][4.0]['ITO'],\n",
    "                            groupCount['1'][4.0]['ITO'],\n",
    "                            groupCount['2'][4.0]['ITO'],\n",
    "                            groupCount['3'][4.0]['ITO']],\n",
    "                     name='16.6-19.2%',marker_color=colormap_PCE[3],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=2)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['0'][5.0]['ITO'],\n",
    "                            groupCount['1'][5.0]['ITO'],\n",
    "                            val_14,\n",
    "                            val_9],\n",
    "                     name='> 19.2%',marker_color=colormap_PCE[4],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=2)\n",
    "\n",
    "\n",
    "fig.update_xaxes(showticklabels=False,title_text=\"n-i-p\", row=1, col=1, range=[1100,0])\n",
    "fig.update_xaxes(showticklabels=False,title_text=\"p-i-n\", row=1, col=2, range=[0,1100])\n",
    "\n",
    "# fig.update_xaxes(showticklabels=False,title_text=\"n-i-p\", row=1, col=1, range=[500,0])\n",
    "# fig.update_xaxes(showticklabels=False,title_text=\"p-i-n\", row=1, col=2, range=[0,500])\n",
    "\n",
    "fig.update_layout(barmode='stack',font_family='Arial', showlegend=False, #barmode='stack'\n",
    "                  xaxis1={'side': 'top'},\n",
    "                  xaxis2={'side': 'top'},)\n",
    "\n",
    "pio.write_image(fig, filedirname+'butterfly_arch_cluster_based.png', width=1*600, height=0.75*600, scale=12)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9fbf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCE-based, percentage\n",
    "\n",
    "colormap_PCE = ['#034e7b', '#0570b0', '#3690c0', '#74a9cf', '#a6bddb']\n",
    "colormap_cluster = ['#045a8d', '#2b8cbe', '#74a9cf', '#bdc9e1']\n",
    "\n",
    "colormap_cluster = [px.colors.qualitative.Antique[4],\n",
    "                    px.colors.qualitative.Antique[9],\n",
    "                    px.colors.qualitative.Antique[6],\n",
    "                    px.colors.qualitative.Antique[8]]\n",
    "\n",
    "opacity = 0.75\n",
    "width = 0.5\n",
    "\n",
    "x = ['< 10%', '10-14%','14-16.6%','16.6-19.2%', '> 19.2%']\n",
    "\n",
    "n_cluster = result_arch['Cluster'].nunique()\n",
    "groupCount = result_arch.groupby(['Cluster','PCE_before_x','FrontContact'])['Cluster'].count()\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    shared_xaxes=False,\n",
    "    shared_yaxes=True,\n",
    "    horizontal_spacing = 0.005,\n",
    "    )\n",
    "    \n",
    "# n-i-p\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['0'][1.0]['FTO'],\n",
    "                            groupCount['0'][2.0]['FTO'],\n",
    "                            val_11,\n",
    "                            val_12,\n",
    "                            val_13],\n",
    "                     name='Cluster 1',marker_color=colormap_PCE[0],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=1)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['1'][1.0]['FTO'],\n",
    "                            groupCount['1'][2.0]['FTO'],\n",
    "                            val_5,\n",
    "                            val_1,\n",
    "                            val_6],\n",
    "                     name='Cluster 2',marker_color=colormap_PCE[1],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=1)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['2'][1.0]['FTO'],\n",
    "                            groupCount['2'][2.0]['FTO'],\n",
    "                            val_2,\n",
    "                            val_3,\n",
    "                            val_7],\n",
    "                     name='Cluster 3',marker_color=colormap_PCE[2],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=1)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['3'][1.0]['FTO'],\n",
    "                            groupCount['3'][2.0]['FTO'],\n",
    "                            val_10,\n",
    "                            val_4,\n",
    "                            val_8],\n",
    "                     name='Cluster 4',marker_color=colormap_PCE[3],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=1)\n",
    "\n",
    "# p-i-n\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['0'][1.0]['ITO'],\n",
    "                            groupCount['0'][2.0]['ITO'],\n",
    "                            groupCount['0'][3.0]['ITO'],\n",
    "                            groupCount['0'][4.0]['ITO'],\n",
    "                            groupCount['0'][5.0]['ITO']],\n",
    "                     name='Cluster 1',marker_color=colormap_PCE[0],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=2)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['1'][1.0]['ITO'],\n",
    "                            groupCount['1'][2.0]['ITO'],\n",
    "                            groupCount['1'][3.0]['ITO'],\n",
    "                            groupCount['1'][4.0]['ITO'],\n",
    "                            groupCount['1'][5.0]['ITO']],\n",
    "                     name='Cluster 2',marker_color=colormap_PCE[1],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=2)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['2'][1.0]['ITO'],\n",
    "                            groupCount['2'][2.0]['ITO'],\n",
    "                            groupCount['2'][3.0]['ITO'],\n",
    "                            groupCount['2'][4.0]['ITO'],\n",
    "                            val_14],\n",
    "                     name='Cluster 3',marker_color=colormap_PCE[2],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=2)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['3'][1.0]['ITO'],\n",
    "                            groupCount['3'][2.0]['ITO'],\n",
    "                            groupCount['3'][3.0]['ITO'],\n",
    "                            groupCount['3'][4.0]['ITO'],\n",
    "                            val_9],\n",
    "                     name='Cluster 4',marker_color=colormap_PCE[3],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=2)\n",
    "\n",
    "fig.update_xaxes(showticklabels=False,title_text=\"n-i-p\", row=1, col=1, range=[1.05,0])\n",
    "fig.update_xaxes(showticklabels=False,title_text=\"p-i-n\", row=1, col=2, range=[0,1.05])\n",
    "\n",
    "fig.update_layout(barmode='relative', barnorm='fraction',font_family='Arial', showlegend=False, #barmode='stack'\n",
    "                  xaxis1={'side': 'top'},\n",
    "                  xaxis2={'side': 'top'},)\n",
    "\n",
    "pio.write_image(fig, filedirname+'butterfly_arch_PCE_based_percent.png', width=1*600, height=0.75*600, scale=12)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8590506a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### PCE-based, count\n",
    "\n",
    "colormap_PCE = ['#034e7b', '#0570b0', '#3690c0', '#74a9cf', '#a6bddb']\n",
    "# colormap_cluster = ['#045a8d', '#2b8cbe', '#74a9cf', '#bdc9e1']\n",
    "\n",
    "colormap_cluster = [px.colors.qualitative.Antique[4],\n",
    "                    px.colors.qualitative.Antique[9],\n",
    "                    px.colors.qualitative.Antique[6],\n",
    "                    px.colors.qualitative.Antique[8]]\n",
    "\n",
    "opacity = 0.75\n",
    "width = 0.5\n",
    "\n",
    "x = ['< 10%', '10-14%','14-16.6%','16.6-19.2%', '> 19.2%']\n",
    "\n",
    "n_cluster = result_arch['Cluster'].nunique()\n",
    "groupCount = result_arch.groupby(['Cluster','PCE_before_x','FrontContact'])['Cluster'].count()\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    shared_xaxes=False,\n",
    "    shared_yaxes=True,\n",
    "    horizontal_spacing = 0.005,\n",
    "    )\n",
    "\n",
    "# n-i-p\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['0'][1.0]['FTO'],\n",
    "                            groupCount['0'][2.0]['FTO'],\n",
    "                            val_11,\n",
    "                            val_12,\n",
    "                            val_13],\n",
    "                     name='Cluster 1',marker_color=colormap_cluster[0],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=1)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['1'][1.0]['FTO'],\n",
    "                            groupCount['1'][2.0]['FTO'],\n",
    "                            val_5,\n",
    "                            val_1,\n",
    "                            val_6],\n",
    "                     name='Cluster 2',marker_color=colormap_cluster[1],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=1)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['2'][1.0]['FTO'],\n",
    "                            groupCount['2'][2.0]['FTO'],\n",
    "                            val_2,\n",
    "                            val_3,\n",
    "                            val_7],\n",
    "                     name='Cluster 3',marker_color=colormap_cluster[2],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=1)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['3'][1.0]['FTO'],\n",
    "                            groupCount['3'][2.0]['FTO'],\n",
    "                            val_10,\n",
    "                            val_4,\n",
    "                            val_8],\n",
    "                     name='Cluster 4',marker_color=colormap_cluster[3],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=1)\n",
    "\n",
    "# p-i-n\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['0'][1.0]['ITO'],\n",
    "                            groupCount['0'][2.0]['ITO'],\n",
    "                            groupCount['0'][3.0]['ITO'],\n",
    "                            groupCount['0'][4.0]['ITO'],\n",
    "                            groupCount['0'][5.0]['ITO']],\n",
    "                     name='Cluster 1',marker_color=colormap_cluster[0],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=2)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['1'][1.0]['ITO'],\n",
    "                            groupCount['1'][2.0]['ITO'],\n",
    "                            groupCount['1'][3.0]['ITO'],\n",
    "                            groupCount['1'][4.0]['ITO'],\n",
    "                            groupCount['1'][5.0]['ITO']],\n",
    "                     name='Cluster 2',marker_color=colormap_cluster[1],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=2)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['2'][1.0]['ITO'],\n",
    "                            groupCount['2'][2.0]['ITO'],\n",
    "                            groupCount['2'][3.0]['ITO'],\n",
    "                            groupCount['2'][4.0]['ITO'],\n",
    "                            val_14],\n",
    "                     name='Cluster 3',marker_color=colormap_cluster[2],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=2)\n",
    "\n",
    "fig.add_trace(go.Bar(y=x,x=[groupCount['3'][1.0]['ITO'],\n",
    "                            groupCount['3'][2.0]['ITO'],\n",
    "                            groupCount['3'][3.0]['ITO'],\n",
    "                            groupCount['3'][4.0]['ITO'],\n",
    "                            val_9],\n",
    "                     name='Cluster 4',marker_color=colormap_cluster[3],\n",
    "                     opacity=opacity,orientation='h', width=width),\n",
    "              row=1,col=2)\n",
    "\n",
    "fig.update_xaxes(showticklabels=False,title_text=\"n-i-p\", row=1, col=1, range=[500,0])\n",
    "fig.update_xaxes(showticklabels=False,title_text=\"p-i-n\", row=1, col=2, range=[0,500])\n",
    "\n",
    "# fig.update_xaxes(showticklabels=False,title_text=\"n-i-p\", row=1, col=1, range=[400,0])\n",
    "# fig.update_xaxes(showticklabels=False,title_text=\"p-i-n\", row=1, col=2, range=[0,400])\n",
    "\n",
    "fig.update_layout(barmode='stack', font_family='Arial', showlegend=False, #barmode='stack'\n",
    "                  xaxis1={'side': 'top'},\n",
    "                  xaxis2={'side': 'top'},)\n",
    "\n",
    "pio.write_image(fig, filedirname+'butterfly_arch_PCE_based.png', width=1*600, height=0.75*600, scale=12)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b3d831",
   "metadata": {},
   "source": [
    "### 3.4. Looking at both clusters and max. PCE group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d65aa9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load data for the \n",
    "cluster_PCE = pd.read_csv(filedirname+'architecture.csv').drop(['Unnamed: 0'],axis=1)\n",
    "cluster_PCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c2c733",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cluster list:')\n",
    "cluster_PCE['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9289d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_PCE['Series'].value_counts()\n",
    "cluster_PCE['PCE_before_ceil_x'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903e4376",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add cluster data to the PCE_df\n",
    "PCE_df_sorted = ((PCE_df).sort_index()).reset_index(drop=True)\n",
    "PCE_df_sorted['Cluster'] = cluster_PCE['Cluster']\n",
    "PCE_df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b034f4dc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "import colorlover as cl\n",
    "from plotly.colors import n_colors\n",
    "import matplotlib\n",
    "import random\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# a = ['PCE 0-8%','PCE 8-12%','PCE 12-16%','PCE 16-20%','PCE 20-24%','PCE24-28%']\n",
    "# a = ['PCE < 10%','PCE 10-14%','PCE 14-17%','PCE 17-20%', 'PCE > 20%']\n",
    "a = ['PCE < 10%','PCE 10-14%','PCE 14-16.6%','PCE 16.6-19.2%', 'PCE > 19.2%']\n",
    "\n",
    "colors = n_colors('rgb(8,29,88)', 'rgb(127,205,187)', 6, colortype='rgb')\n",
    "colors_box = n_colors('rgb(2,7,22)', 'rgb(30,50,45)', 6, colortype='rgb')\n",
    "colors_line = n_colors('rgb(0,5,15)', 'rgb(15,25,23)', 6, colortype='rgb')\n",
    "colors_scatter = n_colors('rgb(0,109,44)', 'rgb(178,226,226)', 4, colortype='rgb')\n",
    "\n",
    "# Plot the violin plot on the background\n",
    "for (i,color,color_line) in zip(unique_ceil, colors, colors_line):\n",
    "    fig.add_trace(go.Violin(x=(PCE_df['PCE_before_x'][PCE_df['PCE_before_ceil_x'] == i])+0,\n",
    "                            y=PCE_df['PCE_delta'][PCE_df['PCE_before_ceil_x'] == i],\n",
    "                            box_visible=False,\n",
    "                            fillcolor = color,\n",
    "                            opacity = 0.4,\n",
    "                            line = dict(color=color_line),\n",
    "                            jitter=True,\n",
    "                            meanline_visible=True\n",
    "                           )\n",
    "                 )\n",
    "\n",
    "# Plot the scattered points\n",
    "for (i) in zip(unique_ceil):\n",
    "    x = cluster_PCE['PCE_before_x'][cluster_PCE['PCE_before_ceil_x'] == i]\n",
    "    fig.add_trace(go.Scatter(x= x + 0.65*np.random.rand(len(x))-0.325, #-0.5 to center it at 0\n",
    "                             y=PCE_df_sorted['PCE_delta'][PCE_df_sorted['PCE_before_ceil_x'] == i],\n",
    "                             mode='markers',\n",
    "                             marker_color=cluster_PCE['Cluster'],\n",
    "                             marker_colorscale='deep_r',\n",
    "                             marker_size=6,\n",
    "                             opacity = 0.8,\n",
    "                            )\n",
    "                 )\n",
    "\n",
    "\n",
    "# Update properties of the figure\n",
    "fig.update_layout(xaxis_title=\"Maximum PCE group\",\n",
    "                  yaxis_title=\"Relative change in max. PCE (after 150 hrs.) (%)\",\n",
    "                  boxgap = 0.85,\n",
    "                  font_family='Arial',\n",
    "                  showlegend=False)\n",
    "    \n",
    "fig.show()\n",
    "\n",
    "# Save a figure \n",
    "pio.write_image(fig, filedirname+'group_violin_1.png',\n",
    "                width=600, height=400, scale=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38138e2d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Separate for different cluster\n",
    "\n",
    "import plotly.io as pio\n",
    "import colorlover as cl\n",
    "from plotly.colors import n_colors\n",
    "import matplotlib\n",
    "import random\n",
    "\n",
    "# Colors_scatter\n",
    "colors_scatter=[ImageColor.getcolor(px.colors.qualitative.Antique[4],'RGB'),\n",
    "                ImageColor.getcolor(px.colors.qualitative.Antique[9],'RGB'),\n",
    "                ImageColor.getcolor(px.colors.qualitative.Antique[6],'RGB'),\n",
    "                ImageColor.getcolor(px.colors.qualitative.Antique[8],'RGB')]\n",
    "\n",
    "# Plot for each cluster\n",
    "for cluster in range(4): # Number of clusters: 4\n",
    "    \n",
    "    fig = go.Figure()\n",
    "\n",
    "    colors = n_colors('rgb(8,29,88)', 'rgb(127,205,187)', 6, colortype='rgb')\n",
    "    colors_box = n_colors('rgb(2,7,22)', 'rgb(30,50,45)', 6, colortype='rgb')\n",
    "    colors_line = n_colors('rgb(0,5,15)', 'rgb(15,25,23)', 6, colortype='rgb')\n",
    "\n",
    "    # Plot the violin plot for specific cluster\n",
    "    for (i,color,color_line) in zip(unique_ceil, colors, colors_line):\n",
    "        fig.add_trace(go.Violin(x=(PCE_df['PCE_before_x'][PCE_df['PCE_before_ceil_x'] == i])+0,\n",
    "                                y=PCE_df_sorted['PCE_delta'][(PCE_df_sorted['PCE_before_ceil_x'] == i) & (PCE_df_sorted['Cluster']== cluster)],\n",
    "                                box_visible=False, \n",
    "                                fillcolor = color,\n",
    "                                opacity = 0.4,\n",
    "                                line = dict(color=color_line),\n",
    "                                jitter=True,\n",
    "                                meanline_visible=True\n",
    "                               )\n",
    "                     )\n",
    "    \n",
    "    # Colors scatter    \n",
    "    if cluster == 0:\n",
    "        color_scatter = px.colors.qualitative.Antique[4]#'black'#colors_scatter[0]\n",
    "    elif cluster == 1:\n",
    "        color_scatter = px.colors.qualitative.Antique[9]#'blue'#colors_scatter[1]\n",
    "    elif cluster ==2:\n",
    "        color_scatter = px.colors.qualitative.Antique[6]#'red'#colors_scatter[2]\n",
    "    else:\n",
    "        color_scatter = px.colors.qualitative.Antique[8]#'green'#colors_scatter[3]\n",
    "    \n",
    "    # Plot the scattered data points\n",
    "    for (i) in zip(unique_ceil):\n",
    "        x = cluster_PCE['PCE_before_x'][(cluster_PCE['PCE_before_ceil_x'] == i) & (cluster_PCE['Cluster'] == cluster)]\n",
    "        fig.add_trace(go.Scatter(x= x + 0.65*np.random.rand(len(x))-0.325, #-0.5 to center it at 0\n",
    "                                 y=PCE_df_sorted['PCE_delta'][(PCE_df_sorted['PCE_before_ceil_x'] == i) & (PCE_df_sorted['Cluster']== cluster)],\n",
    "                                 mode='markers',\n",
    "                                 marker_color=color_scatter,\n",
    "                                 marker_size=6,\n",
    "                                 opacity = 0.7,\n",
    "                                )\n",
    "                     )\n",
    "\n",
    "    # Update properties of the figure\n",
    "    fig.update_traces(marker={'size': 4})\n",
    "    \n",
    "    fig.update_layout(xaxis_title=\"Maximum PCE group (%)\",\n",
    "                      yaxis_title=\"Relative change in max. PCE (after 150 hrs.) (%)\",\n",
    "                      boxgap = 0.85,\n",
    "                      font_family='Arial',\n",
    "                      showlegend=False)\n",
    "    fig.update_yaxes(range=[-20,120],showgrid=True)\n",
    "    fig.update_xaxes(showgrid=False,showticklabels=False)\n",
    "    fig.update_layout(hovermode=\"y unified\")\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # Save a figure\n",
    "    pio.write_image(fig, filedirname+'group_violin_cluster_'+str(cluster)+'_2.png',\n",
    "                    width=425, height=400, scale=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65da3541",
   "metadata": {},
   "source": [
    "### 3.5. Plot the trendline of the mean and interquartile\n",
    "\n",
    "This will give us some ideas what is the maximum, feasible PCE with 0% relative change in max. PCE after 150 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9901c7d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "import colorlover as cl\n",
    "from plotly.colors import n_colors\n",
    "import matplotlib\n",
    "\n",
    "##### CALCULATION FOR THE Y-AXIS, THE RELATIVE CHANGE IN MAX.PCE\n",
    "\n",
    "PCE_df_sorted_upto20 = PCE_df_sorted\n",
    "\n",
    "# Add the PCE_before_median_x\n",
    "PCE_df_sorted_upto20['PCE_before_x'] = PCE_df_sorted['PCE_before_x']\n",
    "PCE_df_sorted_upto20['PCE_before_median_x'] = PCE_df_sorted['PCE_before_median_x']\n",
    "PCE_df_sorted_upto20['PCE_before_mean_x'] = PCE_df_sorted['PCE_before_median_x']\n",
    "\n",
    "# Add the PCE_before_mean_x\n",
    "PCE_df_sorted_upto20['PCE_before_mean_x'].loc[PCE_df_sorted['PCE_before_x'] == 1] = PCE_df_sorted['PCE_before'].loc[PCE_df_sorted['PCE_before_x'] == 1].mean()\n",
    "PCE_df_sorted_upto20['PCE_before_mean_x'].loc[PCE_df_sorted['PCE_before_x'] == 2] = PCE_df_sorted['PCE_before'].loc[PCE_df_sorted['PCE_before_x'] == 2].mean()\n",
    "PCE_df_sorted_upto20['PCE_before_mean_x'].loc[PCE_df_sorted['PCE_before_x'] == 3] = PCE_df_sorted['PCE_before'].loc[PCE_df_sorted['PCE_before_x'] == 3].mean()\n",
    "PCE_df_sorted_upto20['PCE_before_mean_x'].loc[PCE_df_sorted['PCE_before_x'] == 4] = PCE_df_sorted['PCE_before'].loc[PCE_df_sorted['PCE_before_x'] == 4].mean()\n",
    "PCE_df_sorted_upto20['PCE_before_mean_x'].loc[PCE_df_sorted['PCE_before_x'] == 5] = PCE_df_sorted['PCE_before'].loc[PCE_df_sorted['PCE_before_x'] == 5].mean()\n",
    "\n",
    "# Extract median\n",
    "PCE_delta_median = (PCE_df_sorted_upto20.groupby('PCE_before_x')['PCE_delta'].median()).to_frame()\n",
    "PCE_delta_median = PCE_delta_median.rename_axis('PCE_before_x').reset_index()\n",
    "PCE_delta_median.rename(columns={'PCE_delta':'PCE_delta_median'},inplace=True)\n",
    "\n",
    "# Extract mean\n",
    "PCE_delta_mean = (PCE_df_sorted_upto20.groupby('PCE_before_x')['PCE_delta'].mean()).to_frame()\n",
    "PCE_delta_mean = PCE_delta_mean.rename_axis('PCE_before_x').reset_index()\n",
    "PCE_delta_mean.rename(columns={'PCE_delta':'PCE_delta_mean'},inplace=True)\n",
    "\n",
    "# Extract 25th and 75th percentile\n",
    "PCE_delta_quartile_1 = PCE_df_sorted_upto20.groupby('PCE_before_x')['PCE_delta'].quantile(0.25).to_frame()\n",
    "PCE_delta_quartile_3 = PCE_df_sorted_upto20.groupby('PCE_before_x')['PCE_delta'].quantile(0.75).to_frame()\n",
    "\n",
    "PCE_delta_quartile_1 = PCE_delta_quartile_1.rename_axis('PCE_before_x').reset_index()\n",
    "PCE_delta_quartile_3 = PCE_delta_quartile_3.rename_axis('PCE_before_x').reset_index()\n",
    "\n",
    "PCE_delta_quartile_1.rename(columns={'PCE_delta':'PCE_delta_25th'},inplace=True)\n",
    "PCE_delta_quartile_3.rename(columns={'PCE_delta':'PCE_delta_75th'},inplace=True)\n",
    "\n",
    "\n",
    "##### CALCULATION FOR THE X-AXIS, THE PCE_before_mean, PCE_before_median, PCE_before_25th, PCE_before_75th\n",
    "# Extract median x-axis\n",
    "PCE_before_median = (PCE_df_sorted_upto20.groupby('PCE_before_x')['PCE_before'].median()).to_frame()\n",
    "PCE_before_median = PCE_before_median.rename_axis('PCE_before_x').reset_index()\n",
    "PCE_before_median.rename(columns={'PCE_before':'PCE_before_median'},inplace=True)\n",
    "\n",
    "# Extract mean x-axis\n",
    "PCE_before_mean = (PCE_df_sorted_upto20.groupby('PCE_before_x')['PCE_before'].mean()).to_frame()\n",
    "PCE_before_mean = PCE_before_mean.rename_axis('PCE_before_x').reset_index()\n",
    "PCE_before_mean.rename(columns={'PCE_before':'PCE_before_mean'},inplace=True)\n",
    "\n",
    "# Extract 25th and 75th percentile\n",
    "PCE_before_quartile_1 = PCE_df_sorted_upto20.groupby('PCE_before_x')['PCE_before'].quantile(0.25).to_frame()\n",
    "PCE_before_quartile_3 = PCE_df_sorted_upto20.groupby('PCE_before_x')['PCE_before'].quantile(0.75).to_frame()\n",
    "\n",
    "PCE_before_quartile_1 = PCE_before_quartile_1.rename_axis('PCE_before_x').reset_index()\n",
    "PCE_before_quartile_3 = PCE_before_quartile_3.rename_axis('PCE_before_x').reset_index()\n",
    "\n",
    "PCE_before_quartile_1.rename(columns={'PCE_before':'PCE_before_25th'},inplace=True)\n",
    "PCE_before_quartile_3.rename(columns={'PCE_before':'PCE_before_75th'},inplace=True)\n",
    "\n",
    "##### Merge dataframe\n",
    "PCE_delta_quartiles = pd.concat([PCE_delta_median,\n",
    "                                 PCE_delta_mean.drop(columns=['PCE_before_x']),\n",
    "                                 PCE_delta_quartile_1.drop(columns=['PCE_before_x']),\n",
    "                                 PCE_delta_quartile_3.drop(columns=['PCE_before_x']),\n",
    "                                 PCE_before_median.drop(columns=['PCE_before_x']),\n",
    "                                 PCE_before_mean.drop(columns=['PCE_before_x']),\n",
    "                                 PCE_before_quartile_1.drop(columns=['PCE_before_x']),\n",
    "                                 PCE_before_quartile_3.drop(columns=['PCE_before_x'])],\n",
    "                                axis=1)\n",
    "\n",
    "# Save PCE_delta_quartiles\n",
    "PCE_delta_quartiles.to_csv(filedirname+'PCE_delta_quartiles_inc24.csv')\n",
    "\n",
    "PCE_delta_quartiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd240b9f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "# from sklearn.datasets import make_regression\n",
    "\n",
    "# Doing linear regression on MEDIAN\n",
    "print('For MEDIAN:')\n",
    "X = (PCE_delta_quartiles['PCE_before_median'].to_numpy()).reshape(-1,1)\n",
    "y = (PCE_delta_quartiles['PCE_delta_median'].to_numpy()).reshape(-1,1)\n",
    "\n",
    "# Fitting \n",
    "model= LinearRegression().fit(X,y)\n",
    "model_2 = sm.OLS(y,X).fit()\n",
    "y_hat = model.predict(X)\n",
    "\n",
    "MSE=mean_squared_error(y,y_hat)\n",
    "MAE=median_absolute_error(y,y_hat)\n",
    "\n",
    "print('MSE: ',MSE)\n",
    "print('MAE: ',MAE)\n",
    "print('Model coefficients: ',model.coef_)\n",
    "print('y-intercept: ',model.intercept_)\n",
    "print('x-intercept: ',-model.intercept_/model.coef_)\n",
    "\n",
    "# Prediction\n",
    "combined_array = np.column_stack((X,y_hat))\n",
    "prediction=pd.DataFrame(combined_array,columns=['X','y_hat'])\n",
    "\n",
    "MAE_np = MAE*np.ones(len(combined_array))\n",
    "MSE_np = MSE*np.ones(len(combined_array))\n",
    "\n",
    "# Plot figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plot the regression line\n",
    "fig.add_trace(go.Scatter(x=prediction['X'],y=prediction['y_hat'],\n",
    "                         mode='lines', name='Regression',\n",
    "                         line=dict(dash='dash',color='rgb(116,169,207)')))\n",
    "\n",
    "# Plot the scatter median line\n",
    "fig.add_trace(go.Scatter(x=PCE_delta_quartiles['PCE_before_median'],\n",
    "                         y=PCE_delta_quartiles['PCE_delta_median'],\n",
    "                         error_y=dict(type='data', symmetric=False,\n",
    "                                      array=PCE_delta_quartiles['PCE_delta_75th']-PCE_delta_quartiles['PCE_delta_median'],\n",
    "                                      arrayminus=PCE_delta_quartiles['PCE_delta_median']-PCE_delta_quartiles['PCE_delta_25th']),\n",
    "                         error_x=dict(type='data', symmetric=False,\n",
    "                                      array=PCE_delta_quartiles['PCE_before_75th']-PCE_delta_quartiles['PCE_before_median'],\n",
    "                                      arrayminus=PCE_delta_quartiles['PCE_before_median']-PCE_delta_quartiles['PCE_before_25th']),\n",
    "                         mode='markers',name='Median',\n",
    "                         line=dict(color='rgb(5,112,176)')))\n",
    "\n",
    "# Update figure properties\n",
    "fig.update_layout(xaxis_title=\"Maximum PCE group (%)\",\n",
    "                  yaxis_title=\"Relative change in max. PCE (after 150 hrs.) (%)\",\n",
    "                  font_family='Arial',\n",
    "                  showlegend=True)\n",
    "\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Save a figure of 300dpi\n",
    "pio.write_image(fig, filedirname+'median_PCE_delta_fit_inc24.png',\n",
    "                width=0.7*800, height=0.7*600, scale=12)\n",
    "\n",
    "# Model OLS summary\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c00aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "# from sklearn.datasets import make_regression\n",
    "\n",
    "# Doing linear regression on MEAN\n",
    "print('For MEAN:')\n",
    "X = (PCE_delta_quartiles['PCE_before_mean'].to_numpy()).reshape(-1,1)\n",
    "y = (PCE_delta_quartiles['PCE_delta_mean'].to_numpy()).reshape(-1,1)\n",
    "\n",
    "# Fitting \n",
    "model= LinearRegression().fit(X,y)\n",
    "model_2 = sm.OLS(y,X).fit()\n",
    "y_hat = model.predict(X)\n",
    "\n",
    "MSE=mean_squared_error(y,y_hat)\n",
    "MAE=median_absolute_error(y,y_hat)\n",
    "\n",
    "print('MSE: ',MSE)\n",
    "print('MAE: ',MAE)\n",
    "print('Model coefficients: ',model.coef_)\n",
    "print('y-intercept: ',model.intercept_)\n",
    "print('x-intercept: ',-model.intercept_/model.coef_)\n",
    "\n",
    "# Prediction\n",
    "combined_array = np.column_stack((X,y_hat))\n",
    "prediction=pd.DataFrame(combined_array,columns=['X','y_hat'])\n",
    "\n",
    "MAE_np = MAE*np.ones(len(combined_array))\n",
    "MSE_np = MSE*np.ones(len(combined_array))\n",
    "\n",
    "# Plot figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plot the regression line\n",
    "fig.add_trace(go.Scatter(x=prediction['X'],y=prediction['y_hat'],\n",
    "                         mode='lines', name='Regression',\n",
    "                         line=dict(dash='dash',color='rgb(116,169,207)')))\n",
    "\n",
    "# Plot the scatter median line\n",
    "fig.add_trace(go.Scatter(x=PCE_delta_quartiles['PCE_before_mean'],\n",
    "                         y=PCE_delta_quartiles['PCE_delta_mean'],\n",
    "                         error_y=dict(type='data', symmetric=False,\n",
    "                                      array=PCE_delta_quartiles['PCE_delta_75th']-PCE_delta_quartiles['PCE_delta_mean'],\n",
    "                                      arrayminus=PCE_delta_quartiles['PCE_delta_mean']-PCE_delta_quartiles['PCE_delta_25th']),\n",
    "                         error_x=dict(type='data', symmetric=False,\n",
    "                                      array=PCE_delta_quartiles['PCE_before_75th']-PCE_delta_quartiles['PCE_before_mean'],\n",
    "                                      arrayminus=PCE_delta_quartiles['PCE_before_mean']-PCE_delta_quartiles['PCE_before_25th']),\n",
    "                         mode='markers',name='Mean',\n",
    "                         line=dict(color='rgb(5,112,176)')))\n",
    "\n",
    "# Update figure properties\n",
    "fig.update_layout(xaxis_title=\"Maximum PCE group (%)\",\n",
    "                  yaxis_title=\"Relative change in max. PCE (after 150 hrs.) (%)\",\n",
    "                  font_family='Arial',\n",
    "                  showlegend=True)\n",
    "    \n",
    "fig.show()\n",
    "\n",
    "# Save a figure of 300dpi\n",
    "pio.write_image(fig, filedirname+'mean_PCE_delta_fit_inc24.png',\n",
    "                width=0.7*800, height=0.7*600, scale=12)\n",
    "\n",
    "# Model OLS summary\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a029aa",
   "metadata": {},
   "source": [
    "#### What if we tried using all the scattered points/ not groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2618e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the overview\n",
    "fig = go.Figure(data=go.Scatter(x=PCE_df_sorted['PCE_before'],\n",
    "                                y=PCE_df_sorted['PCE_delta'],\n",
    "                                mode='markers'))\n",
    "fig.update_layout(yaxis=dict(range=[-2.1,2.2]),xaxis_title=\"Maximum PCE (%)\",yaxis_title=\"Relative change in max. PCE (after 150 hrs.) (%)\")\n",
    "fig.update_yaxes(type='log')\n",
    "\n",
    "# To display the figure in the output screen\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68748d59",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the overview and OLS trendline\n",
    "\n",
    "fig = px.scatter(PCE_df_sorted, x='PCE_before', y='PCE_delta',\n",
    "                 trendline='ols', #trendline_scope='overall',\n",
    "#                  trendline_options=dict(log_x=True),\n",
    "                 trendline_color_override='red')\n",
    "\n",
    "fig.update_layout(xaxis_title=\"Maximum PCE (%)\",\n",
    "                  yaxis_title=\"Relative change in max. PCE (after 150 hrs.) (%)\",\n",
    "                  font_family='Arial')\n",
    "\n",
    "# Save a figure of 300dpi\n",
    "pio.write_image(fig, filedirname+'ols_trendline.png',\n",
    "                width=800, height=600, scale=5)\n",
    "\n",
    "# yaxis=dict(range=[-2.1,2.2]),\n",
    "# fig.update_yaxes(type='log')\n",
    "\n",
    "# To display the figure in the output screen\n",
    "fig.show()\n",
    "\n",
    "results = px.get_trendline_results(fig)\n",
    "results.px_fit_results.iloc[0].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2947a8e1",
   "metadata": {},
   "source": [
    "## 4. Time series k-means clustering\n",
    "\n",
    "Read more about k-means clustering here: https://en.wikipedia.org/wiki/K-means_clustering.\n",
    "\n",
    "**input**: clean, pre-processed mySeriesDrop_savgol from above\n",
    "\n",
    "**process**:\n",
    "\n",
    "1. do k-means clustering with the same number of clusters as som\n",
    "2. visualization using PCA: k-means, affinity propagation, and dbscan (this one is too long)\n",
    "3. visualization using t-SNE: k-means, affinity propagation\n",
    "4. plot the cluster distribution\n",
    "5. plot elbow method and silhouette method for optimum number of clusters\n",
    "\n",
    "**output**: \n",
    "1. optimum number for clustering\n",
    "2. 2d map of the data points, and their clusters\n",
    "\n",
    "### 4.1. Direct k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa227d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mySeriesDrop_savgol, uncomment to check:\n",
    "# mySeriesDrop_savgol\n",
    "\n",
    "# Load mySeriesDrop_savgol\n",
    "mySeriesDrop_savgol=np.load('dataset/pkl_complete/20221109_mySeriesDrop_savgol.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f00aa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Using the same number of clusters as the SOM\n",
    "som_x = 2\n",
    "som_y = 2\n",
    "cluster_count= som_x*som_y\n",
    "\n",
    "# K-means clustering\n",
    "km = TimeSeriesKMeans(n_clusters=cluster_count, metric=\"dtw\")\n",
    "labels = km.fit_predict(mySeriesDrop_savgol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b7a34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving models as pickle\n",
    "km.to_pickle(filedirname+'TimeSeriesKMeans_4clusters.pkl')\n",
    "\n",
    "# Save numpy array as .npy instead of .pkl\n",
    "np.save(filedirname+'TimeSeriesKMeans_labels_dtw_4clusters.npy',labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4412768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not, let's load labels and km\n",
    "with open(filedirname+'TimeSeriesKMeans_4clusters.pkl', \"rb\") as fh:\n",
    "    km = pickle.load(fh)\n",
    "labels=np.load(filedirname+'TimeSeriesKMeans_labels_dtw_4clusters.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c5002e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import colorlover as cl\n",
    "from plotly.colors import n_colors\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Now, let's plot the results (based on the savgol/ smoothed )\n",
    "plot_count = som_x\n",
    "\n",
    "# Time\n",
    "time = np.linspace(0,150, 900, endpoint=True)\n",
    "\n",
    "# Plot figure\n",
    "fig = make_subplots(\n",
    "    rows=som_x, cols=som_y,\n",
    "    shared_xaxes=True,\n",
    "    shared_yaxes=True,\n",
    "    vertical_spacing=0.1,\n",
    "    )\n",
    "\n",
    "row_i = 0\n",
    "column_j = 0\n",
    "\n",
    "# For each label there is, plot every series with that label\n",
    "for label in set(labels):\n",
    "    cluster = []\n",
    "    \n",
    "    # Plot for the labels\n",
    "    for i in range(len(labels)):\n",
    "        if(labels[i]==label):\n",
    "            fig.add_trace(go.Scatter(x=time, y=mySeriesDrop_savgol[i],\n",
    "                                     mode='lines',\n",
    "                                     name=f\"Cluster {label}\",\n",
    "                                     line_color='rgba(130,179,196,0.12)',\n",
    "                                     showlegend=False),\n",
    "                          row=row_i+1, col=column_j+1)\n",
    "            cluster.append(mySeriesDrop_savgol[i]) # Append the series to take the average\n",
    "    \n",
    "    # Plot the average within the cluster\n",
    "    if len(cluster) > 0:\n",
    "        fig.add_trace(go.Scatter(x=time,y=np.average(np.vstack(cluster),axis=0),\n",
    "                                 mode='lines',\n",
    "                                 name=f'Cluster {label}',\n",
    "                                 line_color='rgb(57,103,119)',\n",
    "                                 showlegend=False),\n",
    "                      row=row_i+1, col=column_j+1)\n",
    "    \n",
    "    # Go to the next row, column\n",
    "    column_j+=1\n",
    "    if column_j%plot_count == 0:\n",
    "        row_i+=1\n",
    "        column_j=0\n",
    "\n",
    "# Update the figure\n",
    "fig.update_yaxes(range=[-0.1,1.1])#, row=x+1, col=y+1)\n",
    "fig.update_layout(font_family='Arial')\n",
    "\n",
    "# Save the figure\n",
    "pio.write_image(fig, filedirname+'TimeSeriesKMeans_4clusters_dtw.png', width=1*600, height=600, scale=15)\n",
    "      \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050003cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Count for each cluster\n",
    "cluster_c = [len(labels[labels==i]) for i in range(cluster_count)]\n",
    "cluster_n = [\"cluster_\"+str(i) for i in range(cluster_count)]\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "# Plot the bar plot of cluster distribution\n",
    "fig = px.bar(x=cluster_n, y=cluster_c,labels=dict(x='Clusters',y='Count'))\n",
    "\n",
    "fig.update_layout(font_family='Arial')\n",
    "fig.update_traces(marker_color='rgba(57,103,119,0.7)')\n",
    "\n",
    "# Save the figure\n",
    "pio.write_image(fig, filedirname+'distribution_TimeSeriesKMeans_4clusters_v.png',\n",
    "                width=1*400, height=400, scale=16)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e1ac7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Plot horizontal distribution\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    y=cluster_n,\n",
    "    x=cluster_c,\n",
    "    orientation='h',\n",
    "    marker=dict(\n",
    "        color= 'rgba(57,103,119,0.7)',\n",
    "    )\n",
    "))\n",
    "\n",
    "fig.update_layout(font_family='Arial',\n",
    "                  xaxis=dict(title='Count'),\n",
    "                  yaxis=dict(title='Cluster'))\n",
    "\n",
    "# Save the figure\n",
    "pio.write_image(fig, filedirname+'distribution_TimeSeriesKMeans_4clusters_h.png',\n",
    "                width=1*400, height=1*400, scale=16)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1db774e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save the clustering results in one dataframe\n",
    "fancy_names_for_labels = [f\"{label}\" for label in labels]\n",
    "result = pd.DataFrame(zip(mySeries['Filename'],mySeries['Pixel'],mySeries['SampleNumber'],fancy_names_for_labels),\n",
    "                      columns=[\"Series\",'Pixel',\"SampleNumber\",\"Cluster\"]).sort_values(by=\"Cluster\")#.set_index(\"Series\")\n",
    "\n",
    "result['PCE_before_x'] = PCE_df['PCE_before_x']\n",
    "result['PCE_before_ceil_x'] = PCE_df['PCE_before_ceil_x']\n",
    "result['PCE_before_median_x'] = PCE_df['PCE_before_median_x']\n",
    "result['PCE_before_mean_x'] = PCE_df['PCE_before_mean_x']\n",
    "\n",
    "# Save the results as csv\n",
    "(result.sort_index()).to_csv(filedirname+'TimeSeriesKMeans_DTW_4clusters_newdata.csv')\n",
    "(result.sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a282ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the distribution\n",
    "\n",
    "colormap_cluster = {\"0\": \"#045a8d\",\n",
    "                    \"1\": \"#2b8cbe\",\n",
    "                    \"2\": \"#74a9cf\",\n",
    "                    \"3\": \"#bdc9e1\"}\n",
    "\n",
    "# Plot the figure\n",
    "fig = px.histogram(result.sort_values(by=['Cluster','PCE_before_ceil_x']),\n",
    "                   x=\"PCE_before_x\", color='Cluster',\n",
    "                   color_discrete_map=colormap_cluster, opacity=0.75,\n",
    "                  )\n",
    "\n",
    "# fig.update_traces(xbins_size=2)\n",
    "\n",
    "fig.update_layout(xaxis_title=\"Max. PCE Group\",#xaxis=dict(range=[9,26]),\n",
    "                  yaxis_title=\"Count\", font_family='Arial',barnorm='fraction',\n",
    "                  bargap=0.1)\n",
    "\n",
    "pio.write_image(fig, filedirname+'TimeSeriesKMeans_4clusters_fraction_3.png',\n",
    "                width=1*400, height=1*400, scale=16)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff1371",
   "metadata": {},
   "source": [
    "### 4.2. Visualizing results with PCA/ PCA clustering\n",
    "\n",
    "Source: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html\n",
    "\n",
    "#### k-means\n",
    "\n",
    "For n = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9673ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "import math\n",
    "\n",
    "data = mySeriesDrop_savgol\n",
    "cluster_count = 9\n",
    "\n",
    "# Data dimensionality reduction\n",
    "reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "print(\"Data dimensionality reduction: done\")\n",
    "        \n",
    "# Loop over cluster_count\n",
    "for cluster in range(cluster_count):\n",
    "    # Only runs for cluster >=2\n",
    "    if cluster >=2:\n",
    "        print('Cluster ', cluster,' is running')       \n",
    "        \n",
    "        # K-means clustering\n",
    "        kmeans = KMeans(init=\"k-means++\", n_clusters=cluster, n_init=4)\n",
    "        print(\"k-means initialization: done\")\n",
    "\n",
    "        # K-means fitting and predicting\n",
    "        labels = kmeans.fit_predict(reduced_data)\n",
    "        print('inertia: ', kmeans.inertia_)\n",
    "        print(\"k-means fitting and predicting: done\")\n",
    "        \n",
    "        # Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "        h = 0.02  # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "        y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "        print(\"creating meshgrid: done\")\n",
    "\n",
    "        # Obtain labels for each point in mesh. Use last trained model.\n",
    "        Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        print(\"prediction for the meshgrid: done\")\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.figure(figsize=(5,5),dpi=300)\n",
    "        plt.clf()\n",
    "        plt.imshow(\n",
    "            Z,\n",
    "            interpolation=\"nearest\",\n",
    "            extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "            cmap=plt.cm.Paired,\n",
    "            aspect=\"auto\",\n",
    "            origin=\"lower\",\n",
    "        )\n",
    "\n",
    "        plt.plot(reduced_data[:, 0], reduced_data[:, 1], \"k.\", markersize=2)\n",
    "\n",
    "        # Plot the centroids as a white X\n",
    "        centroids = kmeans.cluster_centers_\n",
    "        plt.scatter(\n",
    "            centroids[:, 0],\n",
    "            centroids[:, 1],\n",
    "            marker=\"x\",\n",
    "            s=169,\n",
    "            linewidths=3,\n",
    "            color=\"w\",\n",
    "            zorder=10,\n",
    "        )\n",
    "\n",
    "        plt.xlim(x_min, x_max)\n",
    "        plt.ylim(y_min, y_max)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        \n",
    "        # Save figure\n",
    "        plt.show()\n",
    "        plt.savefig(filedirname+'pca_kmeans_dtw_'+str(cluster)+'_cluster.png',\n",
    "                    dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca51cd9",
   "metadata": {},
   "source": [
    "For 1 specific cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea217bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##### import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "data = mySeriesDrop_savgol\n",
    "cluster_count = 4\n",
    "\n",
    "# Data dimensionality reduction\n",
    "reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "print(\"data dimensionality reduction: done\")\n",
    "\n",
    "# K-means clustering\n",
    "kmeans = KMeans(init=\"k-means++\", n_clusters=cluster_count, n_init=4)\n",
    "print(\"k-means initialization: done\")\n",
    "\n",
    "# K-means fitting and predicting\n",
    "labels = kmeans.fit_predict(reduced_data)\n",
    "print('inertia: ', kmeans.inertia_)\n",
    "print(\"k-means fitting and predicting: done\")\n",
    "\n",
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = 0.02  # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "print(\"creating meshgrid: done\")\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "print(\"prediction for the meshgrid: done\")\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(5,5),dpi=300)\n",
    "plt.clf()\n",
    "plt.imshow(\n",
    "    Z,\n",
    "    interpolation=\"nearest\",\n",
    "    extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "    cmap=plt.cm.Paired,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    ")\n",
    "\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], \"k.\", markersize=2)\n",
    "\n",
    "# Plot the centroids as a white X\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(\n",
    "    centroids[:, 0],\n",
    "    centroids[:, 1],\n",
    "    marker=\"x\",\n",
    "    s=169,\n",
    "    linewidths=3,\n",
    "    color=\"w\",\n",
    "    zorder=10,\n",
    ")\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37da27df",
   "metadata": {},
   "source": [
    "#### Affinity propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a4da3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##### import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn import metrics\n",
    "\n",
    "data = mySeriesDrop_savgol\n",
    "cluster_count = 4\n",
    "\n",
    "# Data dimensionality reduction\n",
    "reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "print(\"data dimensionality reduction: done\")\n",
    "\n",
    "# Affinity propagation initialization\n",
    "AP = AffinityPropagation(preference=-50, random_state=0,\n",
    "                         damping = 0.8, max_iter = 1000)\n",
    "print(\"affinity propagation initialization: done\")\n",
    "\n",
    "# Affinity propagation fitting and prediction\n",
    "labels = AP.fit_predict(reduced_data)\n",
    "print(\"affinity propagation fitting and predicting: done\")\n",
    "\n",
    "cluster_centers_indices = AP.cluster_centers_indices_\n",
    "labels = AP.labels_\n",
    "\n",
    "n_clusters_ = len(cluster_centers_indices)\n",
    "\n",
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = 0.02  # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "print(\"creating meshgrid: done\")\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = AP.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "print(\"prediction for the meshgrid: done\")\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(5,5),dpi=300)\n",
    "plt.clf()\n",
    "plt.imshow(\n",
    "    Z,\n",
    "    interpolation=\"nearest\",\n",
    "    extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "    cmap=plt.cm.Paired,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    ")\n",
    "\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], \"k.\", markersize=2)\n",
    "\n",
    "# Plot the centroids as a white X\n",
    "# centroids = kmeans.cluster_centers_\n",
    "# plt.scatter(\n",
    "#     cluster_centers_indices[:, 0],\n",
    "#     cluster_centers_indices[:, 1],\n",
    "#     marker=\"x\",\n",
    "#     s=169,\n",
    "#     linewidths=3,\n",
    "#     color=\"w\",\n",
    "#     zorder=10,\n",
    "# )\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\n",
    "    \"Silhouette Coefficient: %0.3f\"\n",
    "    % metrics.silhouette_score(reduced_data, labels, metric=\"sqeuclidean\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ecffa4",
   "metadata": {},
   "source": [
    "#### DBSCAN\n",
    "\n",
    "DBSCAN takes a long time and I haven't successfully done it from the PC (it probably needs a more powerful computer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3067a8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "\n",
    "data = mySeriesDrop_savgol\n",
    "cluster_count = 4\n",
    "\n",
    "# Data dimensionality reduction\n",
    "reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "print(\"data dimensionality reduction: done\")\n",
    "\n",
    "# DBSCAN initialization\n",
    "db = DBSCAN(eps=0.5, min_samples=10)\n",
    "print(\"dbscan initialization: done\")\n",
    "\n",
    "# DBCAN fitting and prediction\n",
    "labels = db.fit_predict(reduced_data)\n",
    "print(\"DBSCAN fitting and predicting: done\")\n",
    "\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = 0.02  # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "print(\"block4\")\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = db.fit_predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "print(\"prediction for the meshgrid: done\")\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(5,5),dpi=300)\n",
    "plt.clf()\n",
    "plt.imshow(\n",
    "    Z,\n",
    "    interpolation=\"nearest\",\n",
    "    extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "    cmap=plt.cm.Paired,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    ")\n",
    "\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], \"k.\", markersize=2)\n",
    "\n",
    "# Plot the centroids as a white X\n",
    "# centroids = kmeans.cluster_centers_\n",
    "# plt.scatter(\n",
    "#     cluster_centers_indices[:, 0],\n",
    "#     cluster_centers_indices[:, 1],\n",
    "#     marker=\"x\",\n",
    "#     s=169,\n",
    "#     linewidths=3,\n",
    "#     color=\"w\",\n",
    "#     zorder=10,\n",
    "# )\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "\n",
    "print(\n",
    "    \"Silhouette Coefficient: %0.3f\"\n",
    "    % metrics.silhouette_score(reduced_data, labels, metric=\"sqeuclidean\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c175b70",
   "metadata": {},
   "source": [
    "### 4.3. Visualizing results with t-SNE/ t-SNE clustering \n",
    "\n",
    "Source: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html\n",
    "\n",
    "#### k-means\n",
    "\n",
    "For all clusters up to n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e850e05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### Generate all cluster counts\n",
    "import math\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import metrics\n",
    "\n",
    "data = mySeriesDrop_savgol\n",
    "cluster_count = 9\n",
    "\n",
    "# Data dimensionality reduction\n",
    "reduced_data = TSNE(n_components=2).fit_transform(data)\n",
    "# reduced_data = reduced_data.astype('double')\n",
    "print(\"Data dimensionality reduction: done\")\n",
    "        \n",
    "# Loop over cluster_count\n",
    "for cluster in range(cluster_count):\n",
    "    # Only runs for cluster >=2\n",
    "    if cluster >=2:\n",
    "        print('Cluster ', cluster,' is running')       \n",
    "        \n",
    "        # K-means clustering\n",
    "        kmeans = KMeans(init=\"k-means++\", n_clusters=cluster, n_init=4)\n",
    "        print(\"k-means initialization: done\")\n",
    "\n",
    "        # K-means fitting and predicting\n",
    "        labels = kmeans.fit_predict(reduced_data)\n",
    "        print('inertia: ', kmeans.inertia_)\n",
    "        print(\"k-means fitting and predicting: done\")\n",
    "        \n",
    "        # Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "        h = 0.02  # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "        y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "        print(\"creating meshgrid: done\")\n",
    "\n",
    "        # Obtain labels for each point in mesh. Use last trained model.\n",
    "#         xx_double = np.array(xx, dtype=np.double)\n",
    "#         yy_double = np.array(yy, dtype=np.double)\n",
    "        Z = kmeans.predict((np.c_[xx.ravel(), yy.ravel()]).astype('float32'))\n",
    "        print(\"prediction for the meshgrid: done\")\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.figure(figsize=(5,5),dpi=300)\n",
    "        plt.clf()\n",
    "        plt.imshow(\n",
    "            Z,\n",
    "            interpolation=\"nearest\",\n",
    "            extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "            cmap=plt.cm.Paired,\n",
    "            aspect=\"auto\",\n",
    "            origin=\"lower\",\n",
    "        )\n",
    "\n",
    "        plt.plot(reduced_data[:, 0], reduced_data[:, 1], \"k.\", markersize=2)\n",
    "\n",
    "        # Plot the centroids as a white X\n",
    "        centroids = kmeans.cluster_centers_\n",
    "        plt.scatter(\n",
    "            centroids[:, 0],\n",
    "            centroids[:, 1],\n",
    "            marker=\"x\",\n",
    "            s=169,\n",
    "            linewidths=3,\n",
    "            color=\"w\",\n",
    "            zorder=10,\n",
    "        )\n",
    "\n",
    "        plt.xlim(x_min, x_max)\n",
    "        plt.ylim(y_min, y_max)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        \n",
    "        # Save figure\n",
    "        plt.show()\n",
    "        plt.savefig(filedirname+'tsne_kmeans_dtw_'+str(cluster)+'_cluster.png',\n",
    "                    dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c9ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Just one cluster_count\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import metrics\n",
    "\n",
    "data = mySeriesDrop_savgol\n",
    "cluster_count = 4\n",
    "\n",
    "# Data dimensionality reduction\n",
    "reduced_data = TSNE(n_components=2).fit_transform(data)\n",
    "print(\"data dimensionality reduction: done\")\n",
    "\n",
    "# K-means initialization\n",
    "kmeans = KMeans(init=\"k-means++\", n_clusters=cluster_count, n_init=4)\n",
    "print(\"k-means initialization: done\")\n",
    "\n",
    "# K-means fitting and prediction\n",
    "labels = kmeans.fit_predict(reduced_data)\n",
    "print('inertia: ', kmeans.inertia_)\n",
    "print(\"Silhouette Coefficient: %0.3f\"% metrics.silhouette_score(reduced_data, labels, metric=\"sqeuclidean\"))\n",
    "print(\"k-means fitting and predicting: done\")\n",
    "\n",
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = 0.02  # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "#                     dtype='float32')\n",
    "print(\"creating meshgrid: done\")\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = kmeans.predict((np.c_[xx.ravel(), yy.ravel()]).astype('float32'))#,dtype='float32')\n",
    "print(\"prediction for the meshgrid: done\")\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(5,5),dpi=300)\n",
    "plt.clf()\n",
    "plt.imshow(\n",
    "    Z,\n",
    "    interpolation=\"nearest\",\n",
    "    extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "    cmap=plt.cm.Paired,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    ")\n",
    "\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], \"k.\", markersize=2)\n",
    "\n",
    "# Plot the centroids as a white X\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(\n",
    "    centroids[:, 0],\n",
    "    centroids[:, 1],\n",
    "    marker=\"x\",\n",
    "    s=169,\n",
    "    linewidths=3,\n",
    "    color=\"w\",\n",
    "    zorder=10,\n",
    ")\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048e4c8a",
   "metadata": {},
   "source": [
    "#### Affinity propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn import metrics\n",
    "\n",
    "data = mySeriesDrop_savgol\n",
    "cluster_count = 4\n",
    "\n",
    "# Data dimensionality reduction\n",
    "reduced_data = TSNE(n_components=2).fit_transform(data)\n",
    "print(\"data dimensionality reduction: done\")\n",
    "\n",
    "# Affinity propagation initialization\n",
    "AP = AffinityPropagation(preference=-50, random_state=0,\n",
    "                         damping = 0.5, max_iter = 1000)\n",
    "print(\"affinity propagation initialization: done\")\n",
    "\n",
    "# Affinity propagation fitting and prediction\n",
    "labels = AP.fit_predict(reduced_data)\n",
    "print(\"affinity propagation fitting and predicting: done\")\n",
    "\n",
    "cluster_centers_indices = AP.cluster_centers_indices_\n",
    "labels = AP.labels_\n",
    "\n",
    "n_clusters_ = len(cluster_centers_indices)\n",
    "\n",
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = 0.02  # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "print(\"creating meshgrid: done\")\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = AP.predict((np.c_[xx.ravel(), yy.ravel()]).astype('float32'))\n",
    "print(\"prediction for the meshgrid: done\")\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(5,5),dpi=300)\n",
    "plt.clf()\n",
    "plt.imshow(\n",
    "    Z,\n",
    "    interpolation=\"nearest\",\n",
    "    extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "    cmap= plt.cm.Paired,#plt.cm.Paired,tab20b,Set3,Pastel2\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    ")\n",
    "\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], \"k.\", markersize=2)\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "\n",
    "print(\n",
    "    \"Silhouette Coefficient: %0.3f\"\n",
    "    % metrics.silhouette_score(reduced_data, labels, metric=\"sqeuclidean\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496d0570",
   "metadata": {},
   "source": [
    "#### Visualizing which dataset has which cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74016262",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the overview\n",
    "fig, ax = plt.subplots(figsize=(6,12),dpi=300)\n",
    "grouped = result.groupby(['Series','Cluster']).nunique()['Pixel'].reset_index([0,1])\n",
    "piv_grouped = grouped.pivot(index='Series', columns='Cluster', values='Pixel')\n",
    "ax = sns.heatmap(piv_grouped, cmap='viridis', linewidths=0.5, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2927331",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,10),dpi=300)\n",
    "sns.stripplot(x='Cluster', y='Series', data=result, jitter=True,dodge=True, palette='viridis',orient='h')\n",
    "# plt.xticks(rotation=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a2b012",
   "metadata": {},
   "source": [
    "### 4.4. Elbow plot\n",
    "\n",
    "WCSS: https://www.analyticsvidhya.com/blog/2021/01/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning/#:~:text=Elbow%20Method,-In%20the%20Elbow&text=WCSS%20is%20the%20sum%20of,is%20largest%20when%20K%20%3D%201.\n",
    "\n",
    "WCSS ( Within-Cluster Sum of Square ). WCSS is the sum of squared distance between each point and the centroid in a cluster. When we plot the WCSS with the K value, the plot looks like an Elbow. As the number of clusters increases, the WCSS value will start to decrease. WCSS value is largest when K = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97d0f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# array_sum = np.sum(mySeriesDrop_np)\n",
    "array_sum = np.sum(mySeriesDrop_savgol)\n",
    "array_has_nan = np. isnan(array_sum)\n",
    "print(array_has_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba079fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit #-r 1\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Going through all the cluster range, train and calculate kmeans inertia\n",
    "wcss = []\n",
    "for i in range(2, 11, 2): \n",
    "    print('Number clusters: ',i)\n",
    "    kmeans = TimeSeriesKMeans(n_clusters = i, init = 'k-means++', metric='dtw',random_state = 42)\n",
    "    kmeans.fit_predict(mySeriesDrop_savgol)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    print('kmeans inertia: ', kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516cd9b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot WCSS\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=np.arange(2,11,2),\n",
    "    y=wcss,\n",
    "    mode='lines+markers',\n",
    "    line=dict(color='rgb(5,112,176)'),\n",
    "))\n",
    "\n",
    "fig.update_layout(font_family='Arial',\n",
    "                  xaxis=dict(title='Number of clusters'),\n",
    "                  yaxis=dict(title='WCSS'))\n",
    "\n",
    "# Save the figure\n",
    "pio.write_image(fig, filedirname+'kmeans_wcss_result_cluster_2_10_mySeriesDrop_savgol.png',\n",
    "                width=1*300, height=1*300, scale=16)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601d6fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save wcss as a dataset\n",
    "np.save(filedirname+'kmeans_wcss_result_cluster_2_10_mySeriesDrop_savgol.npy',wcss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d4bf8d",
   "metadata": {},
   "source": [
    "### 4.5. Silhouette value method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c6c4ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples,silhouette_score\n",
    "\n",
    "clusters_range = range(2,11,2)\n",
    "results = []\n",
    "\n",
    "# Going through all the cluster range, train and calculate kmeans inertia\n",
    "for i in clusters_range:\n",
    "    print('Number clusters: ',i)\n",
    "    clusterer = TimeSeriesKMeans(n_clusters = i, init = 'k-means++', metric='dtw',random_state = 42)\n",
    "    cluster_labels = clusterer.fit_predict(mySeriesDrop_savgol)#(mySeriesDrop_np)\n",
    "    silhouette_avg = silhouette_score(mySeriesDrop_savgol,cluster_labels) #(mySeriesDrop_np, cluster_labels) \n",
    "    print('kmeans inertia: ', clusterer.inertia_, ', silhouette_avg: ', silhouette_avg)\n",
    "    results.append([i, silhouette_avg])\n",
    "\n",
    "result = pd.DataFrame(results, columns=['n_clusters','silhouette_score'])\n",
    "pivot_km = pd.pivot_table(result, index='n_clusters', values='silhouette_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65637380",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(pivot_km, annot=True, linewidths=0.5, fmt='.3f',cmap= sns.cm.mako_r) #sns.cm.rocket_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d18923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cd9ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot WCSS\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=result['n_clusters'],\n",
    "    y=result['silhouette_score'],\n",
    "    mode='lines+markers',\n",
    "    line=dict(color='rgb(5,112,176)'),\n",
    "))\n",
    "\n",
    "fig.update_layout(font_family='Arial',\n",
    "                  xaxis=dict(title='Number of clusters'),\n",
    "                  yaxis=dict(title='Silhouette score'))\n",
    "\n",
    "# Save the figure\n",
    "pio.write_image(fig, filedirname+'kmeans_silhouette_result_cluster_2_10_mySeriesDrop_savgol.png',\n",
    "                width=1*300, height=1*300, scale=16)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74aa236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result/ silhouette score as a dataset\n",
    "np.save(filedirname+'kmeans_silhouette_result_cluster_2_10_mySeriesDrop_savgol.npy',result)\n",
    "result.to_csv(filedirname+'kmeans_silhouette_result_cluster_2_10_mySeriesDrop_savgol.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182f2f0d",
   "metadata": {},
   "source": [
    "## 5. Side notes/ figures\n",
    "\n",
    "### 5.1. The schematic of degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb8c0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot schematic of degradation\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "x=np.array([0, 0.25, 0.5,\n",
    "            1, 1.5, 1.75,\n",
    "            2, 5])\n",
    "# x_savgol = savgol_filter(x,1,2)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x,\n",
    "#     y=np.array([4,4.75,5,4.75,4,-6]),\n",
    "    y=np.array([18.7, 19.44, 19.75,\n",
    "                20, 19.75, 19.44,\n",
    "                18.7, 8]),\n",
    "    mode='lines',\n",
    "    line=dict(color='rgb(5,112,176)'),\n",
    "#     marker=dict(\n",
    "#         color= 'rgba(57,103,119,0.7)',\n",
    "#     )\n",
    "))\n",
    "\n",
    "fig.update_layout(font_family='Arial',\n",
    "                  xaxis=dict(title='Degradation time (hours)',\n",
    "                             showticklabels=False),\n",
    "                  yaxis=dict(title='PCE',\n",
    "                             showticklabels=False))\n",
    "\n",
    "# Save the figure\n",
    "pio.write_image(fig, filedirname+'schematic.png',\n",
    "                width=1*450, height=1*450, scale=15)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cb58e4",
   "metadata": {},
   "source": [
    "### 5.2. The date range of degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bd337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .pkl file consisting the whole dataset\n",
    "with open('dataset/pkl_complete/20221109_mySeries.pkl', \"rb\") as fh:\n",
    "    mySeries = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12f0b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate the rows and create a list of early date\n",
    "\n",
    "dateList = []\n",
    "for index,row in mySeries.iterrows():\n",
    "    dateRow = mySeries['MPPTdata'][index].iloc[0]\n",
    "    dateList.append(dateRow)\n",
    "    \n",
    "# Convert the dateList into pandas dataframe\n",
    "dateListDf = pd.DataFrame(dateList)\n",
    "\n",
    "# Sort by date, so we know the starting point and ending date of our dataset\n",
    "dateListDf.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afec372",
   "metadata": {},
   "source": [
    "### 5.3. The number of each cluster for each max. PCE group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65665a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data of interest\n",
    "name_int = '20221121_run/sigma_0p5_learningrate_0p6/20221121_sigma_0p5_learningrate_0p6_'\n",
    "cluster_PCE_int = pd.read_csv(name_int+'clusters.csv').drop(['Unnamed: 0'],axis=1)\n",
    "PCE_df_group_int = pd.read_csv(name_int+'PCE_df_grouping.csv')\n",
    "PCE_df_group_int_sort = (PCE_df_group_int.sort_values('Unnamed: 0')).reset_index()\n",
    "\n",
    "cluster_PCE_int['PCE_delta']=PCE_df_group_int_sort['PCE_delta']\n",
    "PCE_df_group_int_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc9f023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group of interest calculation\n",
    "\n",
    "for i in range(4): # Clusters\n",
    "    for j in range(5): # PCE_before_x\n",
    "        count = cluster_PCE_int.loc[(cluster_PCE_int['Cluster']==i)&(cluster_PCE_int['PCE_before_x']==j+1)]\n",
    "        print('For cluster: ',i, ' and PCE_before_x: ',j+1,\n",
    "              '# data points:', count.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e4349",
   "metadata": {},
   "source": [
    "### 5.4. Histogram cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265a8930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the data\n",
    "cluster_1 = cluster_PCE_int['PCE_delta'].loc[(cluster_PCE_int['Cluster']==0)]\n",
    "cluster_2 = cluster_PCE_int['PCE_delta'].loc[(cluster_PCE_int['Cluster']==1)]\n",
    "cluster_3 = cluster_PCE_int['PCE_delta'].loc[(cluster_PCE_int['Cluster']==2)]\n",
    "cluster_4 = cluster_PCE_int['PCE_delta'].loc[(cluster_PCE_int['Cluster']==3)]\n",
    "\n",
    "# Colors, labels, and data for plotting\n",
    "hist_data = [cluster_1, cluster_2, cluster_3, cluster_4]\n",
    "group_labels = ['Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4']\n",
    "colors = [px.colors.qualitative.Antique[4],\n",
    "          px.colors.qualitative.Antique[9],\n",
    "          px.colors.qualitative.Antique[6],\n",
    "          px.colors.qualitative.Antique[8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd99bc1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Create distplot with custom bin_size\n",
    "fig = ff.create_distplot(hist_data, group_labels, bin_size=2.5,\n",
    "                         colors=colors)\n",
    "\n",
    "fig.update_layout(font_family='Arial')\n",
    "pio.write_image(fig, name_int+'rug_dist_PCE_delta.png',\n",
    "                width=1*900, height=1*400, scale=12)\n",
    "pio.write_image(fig, name_int+'rug_dist_PCE_delta_2.png',\n",
    "                width=1*600, height=1*400, scale=12)\n",
    "fig.show()\n",
    "\n",
    "# Create distplot inset (the top)\n",
    "fig = ff.create_distplot(hist_data, group_labels, bin_size=2.5,\n",
    "                         colors=colors)\n",
    "\n",
    "fig.update_layout(font_family='Arial')\n",
    "fig.update_xaxes(range=[-2,90])\n",
    "fig.update_yaxes(range=[-0.005,0.07])\n",
    "pio.write_image(fig, name_int+'rug_dist_PCE_delta_inset_top.png',\n",
    "                width=1*900, height=1*400, scale=12)\n",
    "pio.write_image(fig, name_int+'rug_dist_PCE_delta_inset_top_2.png',\n",
    "                width=1*600, height=1*400, scale=12)\n",
    "fig.show()\n",
    "\n",
    "# Create distplot inset (the bottom)\n",
    "fig = ff.create_distplot(hist_data, group_labels, bin_size=2.5,\n",
    "                         colors=colors)\n",
    "\n",
    "fig.update_layout(font_family='Arial')\n",
    "fig.update_xaxes(range=[-2,90])\n",
    "pio.write_image(fig, name_int+'rug_dist_PCE_delta_inset_bottom.png',\n",
    "                width=1*900, height=1*400, scale=12)\n",
    "pio.write_image(fig, name_int+'rug_dist_PCE_delta_inset_bottom_2.png',\n",
    "                width=1*600, height=1*400, scale=12)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
